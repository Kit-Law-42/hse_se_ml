{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"img/logo_hse_black.jpg\"></center>\n",
    "\n",
    "<h1><center>Data Analysis</center></h1>\n",
    "<h3><center>Andrey Shestakov (<a href=\"mailto:avshestakov@hse.ru\">avshestakov@hse.ru</a>)</center></h3>\n",
    "<hr>\n",
    "<h2><center>Boosting.<sup><a href=\"#fn1\" id=\"ref1\">1</a></sup></center></h2>\n",
    "\n",
    "\n",
    "\n",
    "<sup id=\"fn1\">1. Some materials are taken from <a href=\"http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5_%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D1%8B_%D1%80%D0%B0%D1%81%D0%BF%D0%BE%D0%B7%D0%BD%D0%B0%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F_%D0%BE%D0%B1%D1%80%D0%B0%D0%B7%D0%BE%D0%B2_%28%D0%BA%D1%83%D1%80%D1%81_%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D0%B9%2C_%D0%92.%D0%92.%D0%9A%D0%B8%D1%82%D0%BE%D0%B2%29\">machine learning course of Victor Kitov</a></sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import export_graphviz\n",
    "import subprocess\n",
    "\n",
    "plt.style.use('seaborn-talk')\n",
    "plt.rcParams['figure.figsize'] = (12,8)\n",
    "\n",
    "# Для кириллицы на графиках\n",
    "font = {'family': 'Verdana',\n",
    "        'weight': 'normal'}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "from ipywidgets import interact, IntSlider, fixed, FloatSlider\n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's recall previous lecture\n",
    "\n",
    "## Ensembles\n",
    "\n",
    "* Random Forests, Bagging\n",
    "* Stacking, Blending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear ensembles\n",
    "\n",
    "$$\n",
    "F(x)=f_{0}(x)+\\alpha_{1}h_{1}(x)+...+\\alpha_{M}h_{M}(x)\n",
    "$$\n",
    "\n",
    "**Regression:** $\\widehat{y}(x)=F(x)$\n",
    "\n",
    "**Binary classification:** $score(y|x)=F(x),\\,\\widehat{y}(x)= sign(F(x))$\n",
    "* Notation: $h_{1}(x),...h_{M}(x)$ are called *base learners, weak learners, base models*.\n",
    "* Too expensive to optimize $f_{0}(x),h_{1}(x),...h_{M}(x)$ and $\\alpha_{1},...\\alpha_{M}$ jointly for large $M$.\n",
    "* May lead to overfitting\n",
    "* Idea: optimize $f_{0}(x)$ and then each pair $(h_{m}(x),\\,\\alpha_{m})$ greedily.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Forward stagewise additive modeling (FSAM)\n",
    "\n",
    "**Input**: \n",
    "* training dataset $(x_{i},y_{i}),\\,i=1,2,...N$; \n",
    "* loss function $\\mathcal{L}(f,y)$,\n",
    "* general form of \"base learner\" $h(x|\\gamma)$ (dependent from parameter $\\gamma$) \n",
    "* number $M$ of successive additive approximations.\n",
    "\n",
    "**ALGORITHM**:\n",
    "1. Fit initial approximation $f_{0}(x)=\\arg\\min_{f}\\sum_{i=1}^{N}\\mathcal{L}(f(x_{i}),y_{i})$ \n",
    "2. For $m=1,2,...M$:\n",
    "\n",
    "    1. find next best classifier\n",
    "$$\n",
    "(\\alpha_{m},h_{m})=\\arg\\min_{h,\\alpha}\\sum_{i=1}^{N}\\mathcal{L}(f_{m-1}(x_{i})+\\alpha h(x_{i}),\\,y_{i})\n",
    "$$\n",
    "    2. set\n",
    "$$\n",
    "f_{m}(x)=f_{m-1}(x)+\\alpha_{m}h_{m}(x)\n",
    "$$\n",
    "**Output**: approximation function $f_{M}(x)=f_{0}(x)+\\sum_{m=1}^{M}\\alpha_{m}h_{m}(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Comments on FSAM\n",
    "\n",
    "* Number of steps $M$ should be determined by performance on validation set.\n",
    "* Step 1 need not be solved accurately, since its mistakes are expected to be corrected by future base learners.\n",
    "    * we can take $f_{0}(x)=\\arg\\min_{\\beta\\in\\mathbb{R}}\\sum_{i=1}^{N}\\mathcal{L}(\\beta,y_{i})$ or simply $f_{0}(x)\\equiv0$.\n",
    "* By similar reasoning there is no need to solve 2.A accurately\n",
    "    * typically very simple base learners are used such as trees of depth=1,2,3.\n",
    "* For some loss functions, such as $\\mathcal{L}(y,f(x))=e^{-yf(x)}$ we can solve FSAM explicitly.\n",
    "* For general loss functions gradient boosting scheme should be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Adaboost (discrete version): assumptions\n",
    "\n",
    "* binary classification task $y\\in\\{+1,-1\\}$\n",
    "* family of base classifiers $h(x)=h(x|\\gamma)$ where $\\gamma$ is some fixed parametrization.\n",
    "* $h(x)\\in\\{+1,-1\\}$\n",
    "* classification is performed with $\\widehat{y}=sign\\{f_{0}(x)+\\alpha_{1}f_{1}(x)+...+\\alpha_{M}f_{M}(x)\\}$\n",
    "* optimized loss is $\\mathcal{L}(y,f(x))=e^{-yf(x)}$\n",
    "* FSAM is applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exponential loss\n",
    "\n",
    "<center><img src='img/exp-loss.png' width=900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='img/adaboost.png' width=1200></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Adaboost (discrete version): algorithm\n",
    "\n",
    "**Input**: training dataset $(x_{i},y_{i}),\\,i=1,2,...n$; number of additive weak classifiers $M$, a family of weak classifiers $h(x)\\in\\{+1,-1\\}$, trainable on weighted datasets.\n",
    "\n",
    "**ALGORITHM**:\n",
    "1. Initialize observation weights $w_{i}=1/n$, $i=1,2,...n$.\n",
    "2. for $m=1,2,...M$:\n",
    "\n",
    "    1. fit $h^{m}(x)$ to training data using weights $w_{i}$\n",
    "    2. compute weighted misclassification rate:\n",
    "$$\n",
    "E_{m}=\\sum_{i=1}^{N}w_{i}\\mathbb{I}[h^{m}(x_i)\\ne y_{i}]\n",
    "$$\n",
    "    4. compute $\\alpha_{m}=\\frac{1}{2}\\ln\\left((1-E_{m})/E_{m}\\right)$\n",
    "    5. update sample weights:\n",
    "$$\n",
    "w_{i}\\leftarrow \\frac{w_{i}e^{-\\alpha_{m}y_i h^{m}(x_i)}}{W},$$\n",
    "Where $W$ is normalization factor $\\left(W = \\sum_i w_i e^{-\\alpha_m y_i h^m(x_i)}\\right)$\n",
    "\n",
    "\n",
    "**Output**: composite classifier $f(x)=sign\\left(\\sum_{m=1}^{M}\\alpha_{m}h^{m}(x)\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[-2, -1], [-2, 1], [2, -1], [2, 1], [-1, -1], [-1, 1], [1, -1], [1, 1]])\n",
    "y = np.array([-1,-1,-1,-1,1,1,1,1]) \n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "def plot_decision(model, rows=1, columns=3):\n",
    "    fig, ax = plt.subplots(nrows=rows, ncols=columns, figsize=(15,4))\n",
    "    ax = ax.ravel()\n",
    "\n",
    "    xx1, xx2 = np.meshgrid(np.arange(X[:,0].min()-1, X[:,0].max()+1, 0.1),\n",
    "                           np.arange(X[:,1].min()-1, X[:,1].max()+1, 0.1))\n",
    "\n",
    "    yy = model.staged_predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "\n",
    "    for i, y_hat in enumerate(yy):\n",
    "        y_hat = y_hat.reshape(xx1.shape)\n",
    "\n",
    "        ax[i].set_title('iteration %d' % (i+1))\n",
    "        ax[i].contourf(xx1, xx2, y_hat, cmap=plt.cm.Paired)\n",
    "        ax[i].scatter(X[:, 0], X[:, 1], c=y, s=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier(n_estimators=3, algorithm='SAMME', \n",
    "                         base_estimator=DecisionTreeClassifier(max_depth=1))\n",
    "ada.fit(X, y)\n",
    "\n",
    "plot_decision(ada)\n",
    "ada.estimator_weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "def ada_demo(n_est=1):\n",
    "\n",
    "    ada = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1,), n_estimators=n_est, learning_rate=0.1)\n",
    "    ada.fit(X, y)\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "\n",
    "    xx1, xx2 = np.meshgrid(np.arange(-1.5, 2.5, 0.1),\n",
    "                           np.arange(-1, 1.5, 0.1))\n",
    "\n",
    "    y_hat = ada.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "    \n",
    "    y_hat = y_hat.reshape(xx1.shape)\n",
    "    plt.figure(figsize=(17,15))\n",
    "    plt.title('iteration = %d' % n_est )\n",
    "    plt.contourf(xx1, xx2, y_hat, cmap=plt.cm.Paired)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(noise=0.1)\n",
    "plt.figure(figsize=(17,15))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "interact(ada_demo, n_est=IntSlider(min=1, max=150, value=1, step=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Motivation\n",
    "\n",
    "* Problem: For general loss function $L$ FSAM cannot be solved explicitly\n",
    "* Analogy with function minimization: when we can't find optimum explicitly we use numerical methods\n",
    "* Gradient boosting: numerical method for iterative loss minimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient descent algorithm\n",
    "\n",
    "$$\n",
    "F(w)\\to\\min_{w},\\quad w\\in\\mathbb{R}^{N}\n",
    "$$\n",
    "\n",
    "Gradient descend algorithm:\n",
    "\n",
    "**Input**: $\\eta$-parameter, controlling the speed of convergence $M$-number of iterations\n",
    "\n",
    "**ALGORITHM**:\n",
    "* initialize $w$\n",
    "* for $m=1,2,...M$:\n",
    "    * $\\Delta w = \\frac{\\partial F(w)}{\\partial w}$\n",
    "\t* $w = w-\\eta \\Delta w$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Gradient boosting\n",
    "\n",
    "* Now consider $F\\left(f(x_{1}),...f(x_{N})\\right)=\\sum_{n=1}^{N}\\mathcal{L}\\left(f(x_{n}),y_{n}\\right)$\n",
    "* Gradient descent performs pointwise optimization, but we need generalization, so we optimize in space of functions.\n",
    "* Gradient boosting implements modified gradient descent in function space:\n",
    "    * find $z_{i}=-\\frac{\\partial\\mathcal{L}(r,y_{i})}{\\partial r}|_{r=f^{m-1}(x_{i})}$\n",
    "    * fit base learner $h_{m}(x)$ to $\\left\\{ (x_{i},z_{i})\\right\\} _{i=1}^{N}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='img/golf-MSE.png' width=900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src='img/golf-table.svg' width=900></center>\n",
    "\n",
    "Source - http://explained.ai/gradient-boosting/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "def grad_small_demo(n_est=0, max_depth=1, learning_rate=0.1, show_base_pred=False):\n",
    "    \n",
    "    fig, ax = plt.subplots(2,1, figsize=(16,19))\n",
    "    \n",
    "\n",
    "    y = np.array([100, 250, 140, 220])\n",
    "    X = np.array([[10, 21, 7, 18]]).T\n",
    "\n",
    "    x_range = np.linspace(6, 22, 100).reshape((-1,1))\n",
    "\n",
    "    if n_est:\n",
    "        model = GradientBoostingRegressor(learning_rate=learning_rate, n_estimators=50, \n",
    "                                          max_depth=max_depth, random_state=123)\n",
    "        model.fit(X, y)\n",
    "        dt = model.estimators_[n_est-1][0]\n",
    "        if show_base_pred:\n",
    "            if n_est == 1:\n",
    "                y_hat = np.ones(100)*np.mean(y)\n",
    "            else:\n",
    "                y_hat = list(model.staged_predict(x_range))[n_est-2]\n",
    "            y_hat_base = dt.predict(x_range) * learning_rate + y_hat\n",
    "        else:\n",
    "            y_hat = list(model.staged_predict(x_range))[n_est-1]\n",
    "\n",
    "    else:\n",
    "        y_hat = np.ones_like(x_range)*np.mean(y)\n",
    "        y_hat_base = np.ones_like(x_range)*y_hat\n",
    "\n",
    "    ax[0].scatter(X, y)\n",
    "    ax[0].set_xlabel('$x_1$')\n",
    "    ax[0].set_ylabel('$y$')\n",
    "\n",
    "\n",
    "    if show_base_pred:\n",
    "        ax[0].plot(x_range, y_hat, linestyle='-', c='r', label='previous prediction')\n",
    "        ax[0].plot(x_range, y_hat_base, linestyle=':', c='g', label='final prediction')\n",
    "    else:\n",
    "        ax[0].plot(x_range, y_hat, linestyle='-', c='r', label='prediction')\n",
    "\n",
    "    ax[0].legend()\n",
    "\n",
    "    if n_est:\n",
    "        with open('./tree.dot', 'w') as fout:\n",
    "            export_graphviz(dt, out_file=fout, feature_names=['x'], \n",
    "                           filled=True, rounded=True,  special_characters=True)\n",
    "        command = [\"dot\", \"-Tpng\", \"./tree.dot\", \"-o\", \"./tree.png\"]\n",
    "        subprocess.check_call(command)\n",
    "        ax[1].imshow(plt.imread('tree.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "interact(grad_small_demo,\n",
    "         n_est=IntSlider(min=0, max=50, value=0, step=1), \n",
    "         learning_rate=FloatSlider(min=0.1, max=1., value=0.1, step=0.05),\n",
    "         max_depth=IntSlider(min=1, max=5, value=1, step=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient boosting\n",
    "\n",
    "**Input**: training dataset $(x_{i},y_{i}),\\,i=1,2,...N$; loss function $\\mathcal{L}(f,y)$; learning rate $\\nu$ and the number $M$ of successive additive approximations.\n",
    "\n",
    "1. Fit initial approximation $f_{0}(x)$ (might be taken $f_{0}(x)\\equiv0$)\n",
    "2. For each step $m=1,2,...M$:\n",
    "\n",
    "    1. calculate derivatives $z_{i}=-\\frac{\\partial\\mathcal{L}(r,y_{i})}{\\partial r}|_{r=f^{m-1}(x_{i})}$\n",
    "    2. fit $h_{m}$ to $\\{(x_{i},z_{i})\\}_{i=1}^{N}$, for example by solving\n",
    "$$\n",
    "\\sum_{n=1}^{N}(h_{m}(x_{n})-z_{n})^{2}\\to\\min_{h_{m}}\n",
    "$$\n",
    "    4. set $f_{m}(x)=f_{m-1}(x)+\\nu h_{m}(x)$\n",
    "\n",
    "\n",
    "**Output**: approximation function $f_{M}(x)=f_{0}(x)+\\sum_{m=1}^{M}\\nu h_{m}(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* What changes with classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Nothing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='https://gif.cmtt.space/0/comment-media/comment-14190/d5ccd05658.jpg'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "def logloss(y, p):\n",
    "    y = np.c_[1-y, y]\n",
    "    if len(p.shape) != 2:\n",
    "        p = np.c_[1-p, p]\n",
    "    return (-y*np.log(p)).sum(axis=1)\n",
    "\n",
    "def grad(y, p):\n",
    "    if len(p.shape) == 2:\n",
    "        p = p[:,1]\n",
    "    return(y-p)/(p*(1-p))\n",
    "\n",
    "def sigmoid(score):\n",
    "    return 1./(1 + np.exp(-score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "y = np.array([0, 0, 1, 1, 0])\n",
    "X = np.array([\n",
    "    [1,1],\n",
    "    [2,1],\n",
    "    [1,2],\n",
    "    [-1,1],\n",
    "    [-1,0],\n",
    "])\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='flag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "def grad_small_demo_class(n_est=0, max_depth=1, learning_rate=0.1):\n",
    "\n",
    "    fig, ax = plt.subplots(2,1, figsize=(16,19))\n",
    "\n",
    "    y = np.array([0, 0, 1, 1, 0])\n",
    "    X = np.array([\n",
    "        [1,1],\n",
    "        [2,1],\n",
    "        [1,2],\n",
    "        [-1,1],\n",
    "        [-1,0],\n",
    "    ])\n",
    "\n",
    "\n",
    "    xx1, xx2 = np.meshgrid(np.arange(X[:,0].min()-1, X[:,0].max()+1, 0.01),\n",
    "                           np.arange(X[:,1].min()-1, X[:,1].max()+1, 0.01))\n",
    "\n",
    "    XX = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "\n",
    "    if n_est:\n",
    "        model = GradientBoostingClassifier(learning_rate=learning_rate, n_estimators=n_est, \n",
    "                                           max_depth=max_depth, random_state=123)\n",
    "        model.fit(X, y)\n",
    "        dt = model.estimators_[n_est-1][0]\n",
    "\n",
    "        y_hat_score = list(model.staged_decision_function(X))[-1].flatten()\n",
    "\n",
    "        if n_est >= 2:    \n",
    "            y_hat_prev_score = list(model.staged_decision_function(X))[-2].flatten()\n",
    "\n",
    "        else:\n",
    "            y_hat_prev_score = np.ones_like(y)*(-np.log(1./np.mean(y) - 1))\n",
    "    else:\n",
    "        y_hat_score = -np.log(1./np.mean(y) - 1)\n",
    "        y_hat_prev_score = np.ones_like(y)*np.nan\n",
    "\n",
    "    y_hat = sigmoid(y_hat_score)\n",
    "    y_hat_prev = sigmoid(y_hat_prev_score)\n",
    "\n",
    "    loss = logloss(y, y_hat_prev)\n",
    "    gradient = grad(y, y_hat_prev)\n",
    "\n",
    "    df = pd.DataFrame(data={'class': y, \n",
    "                            'score_stage_{}'.format(n_est-1): y_hat_prev_score, \n",
    "                            'prediction_stage_{}'.format(n_est-1): y_hat_prev, \n",
    "                            'logloss': loss, \n",
    "                            'gradient': gradient,\n",
    "                            'score_stage_{}'.format(n_est): y_hat_score, \n",
    "                            'prediction_stage_{}'.format(n_est): y_hat })\n",
    "\n",
    "    display(df)\n",
    "\n",
    "    ax[0].scatter(X[:, 0], X[:, 1], c=y, cmap='flag')\n",
    "    ax[0].set_xlabel('$x_1$')\n",
    "    ax[0].set_ylabel('$x_2$')\n",
    "\n",
    "\n",
    "    if n_est:\n",
    "        y_hat_all = model.predict_proba(XX)[:, 1]\n",
    "        y_hat_all = y_hat_all.reshape(xx1.shape)\n",
    "    else:\n",
    "        y_hat_all = np.ones_like(xx1)*np.mean(y)\n",
    "\n",
    "    ax[0].set_title('iteration = %d' % n_est )\n",
    "    ax[0].contourf(xx1, xx2, y_hat_all, cmap=plt.cm.Paired, alpha=0.1)\n",
    "\n",
    "\n",
    "    if n_est:\n",
    "        with open('./tree.dot', 'w') as fout:\n",
    "            export_graphviz(dt, out_file=fout, feature_names=['x_1', 'x_2'], \n",
    "                           filled=True, rounded=True,  special_characters=True)\n",
    "        command = [\"dot\", \"-Tpng\", \"./tree.dot\", \"-o\", \"./tree.png\"]\n",
    "        subprocess.check_call(command)\n",
    "        ax[1].imshow(plt.imread('tree.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "interact(grad_small_demo_class,\n",
    "         n_est=IntSlider(min=1, max=50, value=1, step=1), \n",
    "         learning_rate=FloatSlider(min=0.1, max=1., value=0.1, step=0.05),\n",
    "         max_depth=IntSlider(min=1, max=5, value=1, step=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "def grad_demo(n_est=1):\n",
    "    \n",
    "    np.random.seed(123)\n",
    "    X = np.random.uniform(-10, 10, 500)\n",
    "\n",
    "    y = np.sin(X)/X + np.random.normal(0, .1, 500)\n",
    "    plt.scatter(X, y)\n",
    "    \n",
    "\n",
    "    gbr = GradientBoostingRegressor(n_estimators=n_est, learning_rate=0.15)\n",
    "    gbr_full = GradientBoostingRegressor(n_estimators=200, learning_rate=0.15)\n",
    "    gbr.fit(X.reshape(-1,1), y)\n",
    "    gbr_full.fit(X.reshape(-1,1), y)\n",
    "    \n",
    "    x_range = np.linspace(X.min(), X.max(), 100).reshape((-1,1))\n",
    "\n",
    "    for y_hat in gbr.staged_predict(x_range):\n",
    "        plt.plot(x_range, y_hat, alpha=0.4, c='g')\n",
    "\n",
    "    y_hat = gbr_full.predict(x_range)\n",
    "    \n",
    "    plt.title('Estimators %d' % n_est)\n",
    "    plt.plot(x_range, y_hat, c='r')\n",
    "    plt.ylim((-0.5, 1.3))\n",
    "    plt.xlim(-11,11)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "interact(grad_demo, n_est=IntSlider(min=1, max=150, value=1, step=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Modified gradient descent algorithm\n",
    "\n",
    "**Input**: $M$-number of iterations\n",
    "\n",
    "**ALGORITHM**:\n",
    "* initialize $w$\n",
    "* for $m=1,2,...M$:\n",
    "    * $\\Delta w = \\frac{\\partial F(w)}{\\partial w}$\n",
    "    * $c^* = \\arg\\min_c F(w-c \\Delta w)$\n",
    "    * $w = w-c^* \\Delta w$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient boosting with optimal coefficient\n",
    "\n",
    "**Input**: training dataset $(x_{i},y_{i}),\\,i=1,2,...N$; loss function $\\mathcal{L}(f,y)$; learning rate $\\nu$ and the number $M$ of successive additive approximations.\n",
    "\n",
    "1. Fit initial approximation $f_{0}(x)$ (might be taken $f_{0}(x)\\equiv0$)\n",
    "2. For each step $m=1,2,...M$:\n",
    "\n",
    "    1. calculate derivatives $z_{i}=-\\frac{\\partial\\mathcal{L}(r,y_{i})}{\\partial r}|_{r=f^{m-1}(x_{i})}$\n",
    "    2. fit $h_{m}$ to $\\{(x_{i},z_{i})\\}_{i=1}^{N}$, for example by solving\n",
    "$$\n",
    "\\sum_{n=1}^{N}(h_{m}(x_{n})-z_{n})^{2}\\to\\min_{h_{m}}\n",
    "$$\n",
    "3. solve univariate optimization problem:\n",
    "$$\n",
    "\\sum_{i=1}^{N}\\mathcal{L}\\left(f_{m-1}(x_{i})+c_{m}h_{m}(x_{i}),y_{i}\\right)\\to\\min_{c_{m}\\in\\mathbb{R}_{+}}\n",
    "$$\n",
    "\n",
    "4. set $f_{m}(x)=f_{m-1}(x)+c_m h_{m}(x)$\n",
    "\n",
    "\n",
    "**Output**: approximation function $f_{M}(x)=f_{0}(x)+\\sum_{m=1}^{M}c_m h_{m}(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient boosting of trees\n",
    "\n",
    "**Input** : training dataset $(x_{i},y_{i}),\\,i=1,2,...N$; loss function $\\mathcal{L}(f,y)$ and the number $M$ of successive additive approximations.\n",
    "\n",
    "1. Fit constant initial approximation $f_{0}(x)$: $f_{0}(x)=\\arg\\min_{\\gamma}\\sum_{i=1}^{N}\\mathcal{L}(\\gamma,\\,y_{i})$\n",
    "2. For each step $m=1,2,...M$:\n",
    "    1. calculate derivatives $z_{i}=-\\frac{\\partial\\mathcal{L}(r,y)}{\\partial r}|_{r=f^{m-1}(x)}$\n",
    "    2. fit regression tree $h^{m}$ on $\\{(x_{i},z_{i})\\}_{i=1}^{N}$ with some loss function, get leaf regions $\\{R_{j}^{m}\\}_{j=1}^{J_{m}}$.\n",
    "    3. for each terminal region $R_{j}^{m}$, $j=1,2,...J_{m}$ solve univariate optimization problem:\n",
    "$$\n",
    "\\gamma_{j}^{m}=\\arg\\min_{\\gamma}\\sum_{x_{i}\\in R_{j}^{m}}\\mathcal{L}(f_{m-1}(x_{i})+\\gamma,\\,y_{i})\n",
    "$$\n",
    "    4. update $f_{m}(x)=f_{m-1}(x)+\\sum_{j=1}^{J_{m}}\\gamma_{j}^{m}\\mathbb{I}[x\\in R_{j}^{m}]$\n",
    "\n",
    "**Output**: approximation function $f_{M}(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Modification of boosting for trees\n",
    "\n",
    "* Compared to first method of gradient boosting, boosting of regression trees finds additive coefficients individually for each terminal region $R_{j}^{m}$, not globally for the whole classifier $h^{m}(x)$.\n",
    "* This is done to increase accuracy: forward stagewise algorithm cannot be applied to find $R_{j}^{m}$, but it can be applied to find $\\gamma_{j}^{m}$, because second task is solvable for arbitrary $L$.\n",
    "* Max leaves $J$\n",
    "    * interaction between no more than $J-1$ terms\n",
    "* $M$ controls underfitting-overfitting tradeoff and selected using validation set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Shrinkage & subsampling\n",
    "\n",
    "* Shrinkage of general GB, step (d):\n",
    "$$\n",
    "f_{m}(x)=f_{m-1}(x)+\\nu c_{m}h_{m}(x)\n",
    "$$\n",
    "* Shrinkage of trees GB, step (d):\n",
    "\n",
    "$$\n",
    "f_{m}(x)=f_{m-1}(x)+\\nu\\sum_{j=1}^{J_{m}}\\gamma_{jm}\\mathbb{I}[x\\in R_{jm}]\n",
    "$$\n",
    "\n",
    "* Comments:\n",
    "    * $\\nu\\in(0,1]$\n",
    "    * $\\nu \\text{ decreases } \\implies M \\text{ increases } $\n",
    "\n",
    "* Subsampling\n",
    "    * increases speed of fitting\n",
    "    * may increase accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Interpretation - partial dependency plots\n",
    "\n",
    "* Problem - we have huge black-box model $\\hat{y}^k = F(x^k_1, x^k_2,\\dots,x^k_p)$\n",
    "* Want to have at least some interpretation\n",
    "* Idea - cosider a single predictor $x_j$\n",
    "    * Find out its influence on prediction after we have \"averaged out\" the influence of all other variables:\n",
    "    \n",
    "$$ \\phi_j(x) = \\frac{1}{N}\\sum_{k=1}^N F(x^k_1, x^k_2,\\dots, x^k_{j-1}, x, x^k_{j+1} \\dots,x^k_p) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='img/partial_dep.png' width=1000></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* [Gradient Boosting PlayGround](http://arogozhnikov.github.io/2016/07/05/gradient_boosting_playground.html)\n",
    "* [LightGBM](https://github.com/Microsoft/LightGBM), [XGBoost](https://github.com/dmlc/xgboost), [CatBoost](https://github.com/catboost/catboost)\n",
    "* [Libs overview](https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db)\n",
    "* [Gradient boosting by Alexander Dyakonov](https://alexanderdyakonov.wordpress.com/2017/06/09/%d0%b3%d1%80%d0%b0%d0%b4%d0%b8%d0%b5%d0%bd%d1%82%d0%bd%d1%8b%d0%b9-%d0%b1%d1%83%d1%81%d1%82%d0%b8%d0%bd%d0%b3/)\n",
    "* [XGBoost video tutorial](http://education.parrotprediction.teachable.com/p/practical-xgboost-in-python)\n",
    "* [GBM tutorial](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/)"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/1f8c4751e12938961e423759861e6e5a"
  },
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "gist": {
   "data": {
    "description": "CloudMail/hse-da-course/raw/lecture-intro/lecture-intro-v01.ipynb",
    "public": false
   },
   "id": "1f8c4751e12938961e423759861e6e5a"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "livereveal": {
   "theme": "serif",
   "transition": "concave"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "513px",
    "width": "253px"
   },
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "toc_position": {
   "height": "32px",
   "left": "9px",
   "right": "1379px",
   "top": "33px",
   "width": "212px"
  },
  "widgets": {
   "state": {
    "54e80d57f79b4bfc934a2b84cf5fe7ba": {
     "views": [
      {
       "cell_index": 47
      }
     ]
    },
    "5fb17a3592634a4fba98446dacd6db43": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "6f6f6ce7b81743308b92966f225862a8": {
     "views": [
      {
       "cell_index": 34
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
