{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/logo_hse_black.jpg\"></center>\n",
    "\n",
    "<h1><center>Data Analysis</center></h1>\n",
    "<h2><center>Seminar: Neural Networks 2</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RND_SEED = 7\n",
    "numpy.random.seed(RND_SEED)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "In this notebook we will recognize hand-written digits using fully-connected Neural Network (NN) and Convolutional Neural Network (CNN). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "Download and read MNIST sample of hand-written digits. Train sample contains 60k monochrome images with size of 28x28 pixels with values in range [0, 255]. There are 10 classes of the digits: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist # import sample download function\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() # dwonload and read MNIST sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes: \n",
      "X_train:  (60000, 28, 28)\n",
      "y_train:  (60000,)\n",
      "X_test:  (10000, 28, 28)\n",
      "y_test:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('Data shapes: ')\n",
    "print('X_train: ', X_train.shape)\n",
    "print('y_train: ', y_train.shape)\n",
    "print('X_test: ', X_test.shape)\n",
    "print('y_test: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train, img_rows, img_cols = X_train.shape\n",
    "n_test, _, _ = X_test.shape\n",
    "\n",
    "n_train, n_test, img_rows, img_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0] # 28 x 28 pixels with values in range [0, 255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "x, y = 3, 3\n",
    "for i in range(x*y):  \n",
    "    plt.subplot(y, x, i+1)\n",
    "    plt.imshow(X_train[i], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale X\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "\n",
    "# Labels one-hot encoding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "n_classes = 10\n",
    "\n",
    "y_train_hot = to_categorical(y_train, n_classes)\n",
    "y_test_hot = to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully-connected NN\n",
    "\n",
    "Consider the following NN with one hidden layer:\n",
    "\n",
    "<center><img src='img/mnist_2layers.png'></center>\n",
    "\n",
    "It has 28x28=784 inputs and 10 output neurons. We will use [Keras](https://keras.io) library to implement this network. But firstly we need to transform 28x28 images into vectors of 784 pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:     (60000, 28, 28)\n",
      "X_train_nn:  (60000, 784)\n"
     ]
    }
   ],
   "source": [
    "X_train_nn = X_train.reshape((n_train, -1, ))\n",
    "X_test_nn = X_test.reshape((n_test, -1, ))\n",
    "\n",
    "print('X_train:    ', X_train.shape)\n",
    "print('X_train_nn: ', X_train_nn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Fit NN Model\n",
    "\n",
    "[Keras](https://keras.io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, Convolution2D, MaxPooling2D\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() # init NN model\n",
    "model.add(Dense(units=15, input_dim=img_rows*img_cols, kernel_initializer=initializers.RandomNormal(), bias_initializer=initializers.Zeros(), activation='sigmoid')) # incert one hidden layer\n",
    "model.add(Dense(n_classes, activation='softmax')) # incert output layer\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.8218 - accuracy: 0.8232 - val_loss: 0.4108 - val_accuracy: 0.9049\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.3524 - accuracy: 0.9102 - val_loss: 0.3002 - val_accuracy: 0.9209\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 26us/sample - loss: 0.2820 - accuracy: 0.9228 - val_loss: 0.2634 - val_accuracy: 0.9264\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.2501 - accuracy: 0.9297 - val_loss: 0.2407 - val_accuracy: 0.9325\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.2299 - accuracy: 0.9343 - val_loss: 0.2280 - val_accuracy: 0.9352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb42802e10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train_nn,\n",
    "          y_train_hot,\n",
    "          batch_size=32,\n",
    "          nb_epoch=5,\n",
    "          validation_data=(X_test_nn, y_test_hot),\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_probas = model.predict_proba(X_test_nn)\n",
    "y_test_pred_classes = model.predict_classes(X_test_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.8966081e-05, 7.2436400e-05, 5.6281094e-03, 1.2271790e-02,\n",
       "        9.5623573e-06, 9.0406288e-04, 5.6775190e-07, 9.7928166e-01,\n",
       "        2.0547275e-04, 1.5573653e-03],\n",
       "       [2.2778818e-03, 2.6913020e-03, 9.8319095e-01, 3.4498668e-03,\n",
       "        3.6027132e-06, 1.2391252e-03, 2.7664155e-03, 7.8349738e-05,\n",
       "        4.2891437e-03, 1.3300636e-05]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted probas for 2 images\n",
    "y_test_pred_probas[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, 0, 4, 1, 4, 9, 6, 9])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted classes for 10 images\n",
    "y_test_pred_classes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.9352\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(y_test, y_test_pred_classes)\n",
    "print(\"Test accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional NN\n",
    "\n",
    "Consider a CNN model with structure similar to this:\n",
    "<center><img src='img/network.png'></center>\n",
    "\n",
    "\n",
    "Where convolution is:\n",
    "<center><img src='img/convolution.jpg' width=\"400\"></center>\n",
    "\n",
    "And pooling is:\n",
    "<center><img src='img/pooling2.png' width=\"400\"></center>\n",
    "\n",
    "The model has 28x28x1 inputs and 10 output neurons. We will use [Keras](https://keras.io) library to implement this network. But firstly we need to transform 28x28 images into array of (28 x 28 x 1) pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.image_data_format() != 'channels_last':\n",
    "    shape_ord = (1, img_rows, img_cols)\n",
    "else:\n",
    "    shape_ord = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:     (60000, 28, 28)\n",
      "X_train_cnn:  (60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train_cnn = X_train.reshape((X_train.shape[0],) + shape_ord)\n",
    "X_test_cnn = X_test.reshape((X_test.shape[0],) + shape_ord)\n",
    "\n",
    "print('X_train:    ', X_train.shape)\n",
    "print('X_train_cnn: ', X_train_cnn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Fit NN Model\n",
    "\n",
    "[Keras](https://keras.io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 10)        100       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 10)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 13, 13, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 10)        910       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 10)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5, 5, 10)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 15)                3765      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                160       \n",
      "=================================================================\n",
      "Total params: 4,935\n",
      "Trainable params: 4,935\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, Convolution2D, MaxPooling2D\n",
    "n_filters = 10 # number of convolutional filters\n",
    "n_conv = 3 # filters size (n_conv x n_conv)\n",
    "n_pool = 2 # pool size (n_pool x n_pool)\n",
    "\n",
    "# Create CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Add convolutional and pooling layers\n",
    "model.add(Convolution2D(n_filters, n_conv, padding='valid', input_shape=shape_ord, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(n_pool, n_pool)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution2D(n_filters, n_conv, padding='valid', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(n_pool, n_pool)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Flatten arrays into vectors before fully-connected layers: (X, Y, Z) -> (XYZ, )\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add fully-connected layers\n",
    "model.add(Dense(15, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 11s 189us/sample - loss: 1.1339 - accuracy: 0.5855 - val_loss: 0.2597 - val_accuracy: 0.9504\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 10s 159us/sample - loss: 0.7635 - accuracy: 0.7170 - val_loss: 0.1770 - val_accuracy: 0.9616\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 9s 157us/sample - loss: 0.6849 - accuracy: 0.7487 - val_loss: 0.1590 - val_accuracy: 0.9661\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 10s 172us/sample - loss: 0.6497 - accuracy: 0.7637 - val_loss: 0.1367 - val_accuracy: 0.9679\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 11s 178us/sample - loss: 0.6280 - accuracy: 0.7727 - val_loss: 0.1249 - val_accuracy: 0.9726\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb43211cf8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_cnn,\n",
    "          y_train_hot,\n",
    "          batch_size=32,\n",
    "          nb_epoch=5,\n",
    "          validation_data=(X_test_cnn, y_test_hot),\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_probas = model.predict_proba(X_test_cnn)\n",
    "y_test_pred_classes = model.predict_classes(X_test_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.9726\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(y_test, y_test_pred_classes)\n",
    "print(\"Test accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 256, 256\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "\n",
    "for image_path in glob.glob('./hot-dog-not-hot-dog/*/*hot_dog/*.jpg'):\n",
    "    try:\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.resize(image, (img_width, img_height))\n",
    "            \n",
    "        images.append(image)\n",
    "        if '/hot_dog' in image_path:\n",
    "            labels.append(1)\n",
    "        if '/not_hot_dog' in image_path:\n",
    "            labels.append(0)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(images).astype(np.float32) / 255.0\n",
    "labels = np.array(labels).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.33, random_state=42)\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    horizontal_flip = True,\n",
    "    fill_mode = \"nearest\",\n",
    "    zoom_range = 0.3,\n",
    "    width_shift_range = 0.3,\n",
    "    height_shift_range=0.3,\n",
    "    rotation_range=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = applications.VGG19(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not fit all layers\n",
    "for layer in model.layers[:5]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "predictions = Dense(2, activation=\"softmax\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final = Model(inputs = model.input, outputs = predictions)\n",
    "\n",
    "model_final.compile(loss = \"categorical_crossentropy\", \n",
    "                    optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), \n",
    "                    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"vgg19_1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', save_freq=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIT = False\n",
    "# don't fit now, becuase it is quite long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FIT:\n",
    "    model_final.fit(datagen.flow(X_train, y_train,\n",
    "                                 batch_size=batch_size),\n",
    "                          epochs=epochs,\n",
    "                          steps_per_epoch=32,\n",
    "                          validation_data=(X_test, y_test),\n",
    "                          workers=4,\n",
    "                          callbacks = [checkpoint, early])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "254px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
