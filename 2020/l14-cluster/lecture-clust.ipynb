{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"img/logo_hse_black.jpg\"></center>\n",
    "\n",
    "<h1><center>Data Analysis</center></h1>\n",
    "<h3><center>Andrey Shestakov (<a href=\"mailto:avshestakov@hse.ru\">avshestakov@hse.ru</a>)</center></h3>\n",
    "<hr>\n",
    "<h2><center>Clustering<sup><a href=\"#fn1\" id=\"ref1\">1</a></sup></center></h2>\n",
    "\n",
    "\n",
    "\n",
    "<sup id=\"fn1\">1. Some materials are taken from <a href=\"http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5_%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D1%8B_%D1%80%D0%B0%D1%81%D0%BF%D0%BE%D0%B7%D0%BD%D0%B0%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F_%D0%BE%D0%B1%D1%80%D0%B0%D0%B7%D0%BE%D0%B2_%28%D0%BA%D1%83%D1%80%D1%81_%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D0%B9%2C_%D0%92.%D0%92.%D0%9A%D0%B8%D1%82%D0%BE%D0%B2%29\">machine learning course of Victor Kitov</a></sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-talk')\n",
    "plt.rcParams['figure.figsize'] = (16,8)\n",
    "\n",
    "# Для кириллицы на графиках\n",
    "font = {'family': 'Verdana',\n",
    "        'weight': 'normal'}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "from ipywidgets import interact, IntSlider, fixed, FloatSlider\n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Aim of clustering\n",
    "\n",
    "Clustering is partitioning of objects into groups so that:\n",
    "* inside groups objects are very similar\n",
    "* objects from different groups are dissimilar\n",
    "\n",
    "<center><img src=\"https://i.ytimg.com/vi/zPJtDohab-g/maxresdefault.jpg\" width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Aim of clustering\n",
    "\n",
    "* Unsupervised learning\n",
    "* No definition of ''similar''\n",
    "    * different algorithms use different formalizations of similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Applications of clustering\n",
    "\n",
    "* data summarization\n",
    "    * feature vector is replaced by cluster number\n",
    "* feature extraction\n",
    "    * cluster number, cluster average target, distance to native cluster center / other clusters\n",
    "* community detection in networks\n",
    "    * nodes - people, similarity - number of connections\n",
    "* outlier detection\n",
    "    * outliers do not belong any cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Groups of clustering methods\n",
    "\n",
    "* Prototype-based\n",
    "    * Replace cluster with ethalon object\n",
    "* Hierarchcal\n",
    "    * Build cluster hierarchy\n",
    "* Density-based\n",
    "    * Find dence groups of objects\n",
    "* Spectral\n",
    "    * Use spectral techniques\n",
    "* Grid-based\n",
    "    * Split feature space in grids and connect them\n",
    "* Probabilistic models\n",
    "    * Assume some mixture of probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Clustering algorithms comparison\n",
    "\n",
    "We can compare clustering algorithms in terms of:\n",
    "* computational complexity\n",
    "* do they build flat or hierarchical clustering?\n",
    "* can the shape of clustering be arbitrary?\n",
    "    * if not is it symmetrical, can clusters be of different size?\n",
    "* can clusters vary in density of contained objects?\n",
    "* robustness to outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prototype-based clustering\n",
    "# K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Prototype-based clustering\n",
    "* Clustering is flat (not hierarchical)\n",
    "* Number of clusters $K$ is specified in advance\n",
    "* Each object $x_{n}$ is associated with a cluster $z_{n}$\n",
    "    * $z_{n} \\in \\{1,\\dots,K\\}$\n",
    "* Each cluster $C_{k}$ is defined by its representative $\\mu_{k}$, $k=1,2,...K.$\n",
    "* Criterion to find representatives $\\mu_{1},...\\mu_{K}$:\n",
    "$$\n",
    "Q(z_{1},...z_{N})=\\sum_{n=1}^{N}\\min_{k}\\rho(x_{n},\\mu_{k})\\to\\min_{\\mu_{1},...\\mu_{K}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generic algorithm\n",
    "\n",
    "<center><img src='img/prot-generic.png' width=700></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Comments\n",
    "\n",
    "* different distance functions lead to different algorithms:\n",
    "    * $\\rho(x,x')=\\left\\lVert x-x'\\right\\rVert _{2}^{2}$=> K-means\n",
    "    * $\\rho(x,x')=\\left\\lVert x-x'\\right\\rVert _{1}$=> K-medians\n",
    "\n",
    "* $\\mu_{k}$ may be arbitrary or constrained to be existing objects\n",
    "* $K$ - is set by user\n",
    "    * if chosen small=>distinct clusters will get merged\n",
    "    * better to take $K$ larger and then merge similar clusters.\n",
    "\n",
    "* Shape of clusters is defined by $\\rho(\\cdot,\\cdot)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example\n",
    "\n",
    "<center><img src='img/Kmeans_animation.gif' width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## K-means algorithm\n",
    "\n",
    "* Suppose we want to cluster our data into $K$ clusters.\n",
    "* Cluster $i$ has a center $\\mu_{i}$, i=1,2,...K.\n",
    "* Consider the task of minimizing \n",
    "$$\n",
    "\\sum_{n=1}^{N}\\left\\lVert x_{n}-\\mu_{z_{n}}\\right\\rVert _{2}^{2}\\to\\min_{z_{1},...z_{N},\\mu_{1},...\\mu_{K}}\\quad (1)\n",
    "$$\n",
    "where $z_{i}\\in\\{1,2,...K\\}$ is cluster assignment for $x_{i}$ and $\\mu_{1},...\\mu_{K}$ are cluster centers.\n",
    "* Direct optimization requires full search and is impractical.\n",
    "* K-means is a suboptimal algorithm for optimizing (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## K-means algorithm\n",
    "\n",
    "<center><img src='img/kmeans-alg.png' width=700></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## K-means properties\n",
    "\n",
    "* **Convergence conditions**\n",
    "    * maximum number of iterations reached\n",
    "    * cluster assignments $z_{1},...z_{N}$ stop to change (exact) \n",
    "    * $\\{\\mu_{i}\\}_{i=1}^{K}$ stop changing significantly (approximate)\n",
    "* **Initialization:**\n",
    "    * typically $\\{\\mu_{i}\\}_{i=1}^{K}$ are initialized to randomly chosen training objects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## K-means properties\n",
    "\n",
    "\n",
    "* **Optimality:**\n",
    "    * criteria is non-convex \n",
    "    * solution depends on starting conditions\n",
    "    * may restart several times from different initializations and select solution giving minimal value of (1).\n",
    "* **Complexity:** $O(NDKI)$\n",
    "    * $K$ is the number of clusters \n",
    "    * $I$ is the number of iterations. \n",
    "* usually few iterations are enough for convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gotchas\n",
    "\n",
    "* K-means assumes that clusters are convex:\n",
    "<center><img src='img/kmeans_digits.png' width=800></center>\n",
    "* It always finds clusters even if none actually exist\n",
    "    * need to control cluster quality metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Main factors of solution\n",
    "* Initial position of centroids\n",
    "* Number of centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Number of centroids - K\n",
    "\n",
    "* Don't use vanilla k-means (X-means, ik-means)\n",
    "* Consider cluster quality measures\n",
    "* Use heuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Elbow method\n",
    "\n",
    "* Criterion of k-means\n",
    "$$ Q(C) =  Q(z_{1},...z_{N}) = \\sum_{n=1}^{N}\\left\\lVert x_{n}-\\mu_{z_{n}}\\right\\rVert _{2}^{2}\\to\\min_{z_{1},...z_{N},\\mu_{1},...\\mu_{K}}\\quad (1) $$\n",
    "* Lets take all possible values of $K$, and chose one that delivers minimum of $Q(C)$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Won't work! Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X, y = make_blobs(n_samples=500,\n",
    "                  n_features=2,\n",
    "                  centers=4,\n",
    "                  cluster_std=1,\n",
    "                  center_box=(-10.0, 10.0),\n",
    "                  shuffle=True,\n",
    "                  random_state=1) \n",
    "\n",
    "\n",
    "crit = []\n",
    "\n",
    "for k in range(2, 8):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=1).fit(X)\n",
    "    crit.append(np.sqrt(kmeans.inertia_))\n",
    "    \n",
    "def elbow_demo(k=2):\n",
    "    \n",
    "    X, y = make_blobs(n_samples=500,\n",
    "                  n_features=2,\n",
    "                  centers=4,\n",
    "                  cluster_std=1,\n",
    "                  center_box=(-10.0, 10.0),\n",
    "                  shuffle=True,\n",
    "                  random_state=1) \n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k, random_state=1).fit(X)\n",
    "    \n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    \n",
    "    ax[0].scatter(X[:,0], X[:,1], c=kmeans.labels_)\n",
    "    \n",
    "    ax[0].scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
    "                  marker='o', c=\"white\", alpha=1, s=200)\n",
    "    \n",
    "    ax[0].set_xlabel('$x_1$')\n",
    "    ax[0].set_ylabel('$x_2$')\n",
    "\n",
    "    for i, c in enumerate(kmeans.cluster_centers_):\n",
    "        ax[0].scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n",
    "        \n",
    "    ax[1].plot(range(2,8), crit, marker='s')\n",
    "    \n",
    "    ax[1].set_xlabel('$k$')\n",
    "    ax[1].set_ylabel('$Q^{(k)}(C)$')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Elbow method\n",
    "\n",
    "* Choose $K$ after which $Q(C)$ stops decreasing rapidly\n",
    "* A bit more formally\n",
    "$$ D(k) = \\frac{|Q^{(k)}(C) - Q^{(k+1)}(C)|}{|Q^{(k-1)}(C) - Q^{(k)}(C)|} \\quad \\text{\" is small \"} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4734028aec5147b58ac1e613ae8246e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=2, description='k', max=8, min=2), Output()), _dom_classes=('widget-inte…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot = interact(elbow_demo, k=IntSlider(min=2,max=8,step=1,value=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Warning!\n",
    "* Heuristics are not dogma!\n",
    "* If at least some clusters are interpretable - this is good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Centroid initialization\n",
    "* Initiallize randombly\n",
    "    * Run multiple times and take result with smallest $Q(C)$\n",
    "* Use result of another clustering method as starting point\n",
    "* k-means++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### K-means++\n",
    "* Choose 1st centroid randomly from initial objects\n",
    "* For each data point calculate its distance to closest centroid $d_{\\min}(x_i) = \\min_{\\mu_j} \\|x_i - \\mu_j\\|^2$\n",
    "* Take point as next centroid with probability $p(x_i) \\propto d_{\\min}(x_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def demo_kmpp(iters=1):\n",
    "\n",
    "    X, y = make_blobs(n_samples=550, cluster_std=1.5, n_features=2, centers=5, random_state=12345)\n",
    "\n",
    "    X_grid1, X_grid2 = np.meshgrid(np.linspace(-12, 18, 500),\n",
    "                                   np.linspace(-11, 8, 500))\n",
    "\n",
    "    XX = np.c_[X_grid1.flatten(), X_grid2.flatten()]\n",
    "    np.random.seed(1)\n",
    "    centroids = np.empty((0, 2))\n",
    "\n",
    "    for i in range(iters):\n",
    "        if i == 0:\n",
    "            d = np.ones_like(y, dtype=float)\n",
    "        else:\n",
    "            d = pairwise_distances(X, centroids, metric='euclidean').min(axis=1)\n",
    "        weights = d/d.sum()\n",
    "\n",
    "        centroid_idx = np.random.choice(X.shape[0], size=1, replace=False, p=weights)[0]\n",
    "        centroids = np.r_[centroids, X[centroid_idx, np.newaxis]]\n",
    "\n",
    "    d_grid = pairwise_distances(XX, centroids, metric='euclidean').min(axis=1)\n",
    "\n",
    "    d_grid = d_grid.reshape(X_grid1.shape)\n",
    "    d_grid = d_grid/d_grid.max()\n",
    "\n",
    "    levels = np.linspace(0, 1, 100)\n",
    "\n",
    "    plt.contourf(X_grid1, X_grid2, d_grid, cmap=plt.cm.Blues, alpha=0.7, levels=levels)\n",
    "    plt.scatter(X[:, 0], X[:, 1], s=100)\n",
    "\n",
    "    centers = centroids\n",
    "    \n",
    "    plt.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=500, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        plt.scatter(c[0], c[1], marker='$%d$' % (i+1), alpha=1,\n",
    "                    s=100, edgecolor='k')\n",
    "\n",
    "    plt.xlabel('$x_1$', fontsize=15)\n",
    "    plt.ylabel('$x_2$', fontsize=15)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37a304c1a894addbf3886cfcd4664e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='iters', max=6, min=1), Output()), _dom_classes=('widget-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.demo_kmpp(iters=1)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(demo_kmpp, iters=IntSlider(min=1,max=6,step=1,value=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## K-means\n",
    "* Not robust to outliers\n",
    "    * K-medians is a bit more robust\n",
    "* K-representatives may create singleton clusters in outliers if centroids get initialized with outlier\n",
    "    * better to init centroids with mean of $m$ randomly chosen objects\n",
    "* Constructs spherical clusters of similar radii\n",
    "    * Allows kernel version which can find non-convex clusters in original space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Insparation - Motivation\n",
    "<center><img src='img/languages.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Insparation - Motivation\n",
    "\n",
    "* Number of clusters $K$ not known a priory.\n",
    "* Clustering is usually not flat, but hierarchical with different levels of granularity:\n",
    "    * sites in the Internet\n",
    "    * books in library\n",
    "    * animals in nature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Hierarchical clustering\n",
    "\n",
    "Hierarchical clustering may be:\n",
    "\n",
    "* top-down\n",
    "    * divisive clustering\n",
    "* bottom-up\n",
    "    * aglomerative clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Aglomerative example\n",
    "\n",
    "<center><img src='img/agg-demo.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dendrogram\n",
    "\n",
    "<center><img src='img/dendro1.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Algorithm\n",
    "\n",
    "<center><img src='img/agg-alg.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* What is \"closest pair of clusters\"?\n",
    "* How do we recalculate distances between clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Distance recalculation\n",
    "\n",
    "* Consider clusters $A=\\{x_{i_{1}},x_{i_{2}},...\\}$ and $B=\\{x_{j_{1}},x_{j_{2}},...\\}$.\n",
    "* We can define the following natural ways to recalculate distances\n",
    "    * nearest neighbour (or single link)\n",
    "    $$\n",
    "    \\rho(A,B)=\\min_{a\\in A,b\\in B}\\rho(a,b)\n",
    "    $$\n",
    "    * furthest neighbour (or complete link)\n",
    "    $$\n",
    "    \\rho(A,B)=\\max_{a\\in A,b\\in B}\\rho(a,b)\n",
    "    $$\n",
    "    * group average link\n",
    "    $$\n",
    "    \\rho(A,B)=\\frac{1}{N_AN_B}\\sum\\limits_{a\\in A,b\\in B}\\rho(a,b)\n",
    "    $$\n",
    "    * closest centroid (or centroid-link)\n",
    "    $$\n",
    "    \\rho(A,B)=\\rho(\\mu_{A},\\mu_{B})\n",
    "    $$\n",
    "    where $\\mu_{U}=\\frac{1}{|U|}\\sum_{x\\in U}x$ or $m_{U}=median_{x\\in U}\\{x\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Lance–Williams formula\n",
    "\n",
    "$$ \\rho(C_i \\cup C_j, C_k) = a_i \\cdot \\rho(C_i, C_k) + a_j \\cdot \\rho(C_j, C_k) + b \\cdot \\rho(C_i, C_j) + c \\cdot |\\rho(C_i, C_k) - \\rho(C_j, C_k)|$$\n",
    "\n",
    "\n",
    "<center><img src='img/lw_coefs.png' width=700></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Heuristics for dendrogram quality\n",
    "\n",
    "* **cophenetic distance** between objects $x_i$ и $x_j$ - height of dendrogram at which whose two objects have merged\n",
    "\n",
    "\n",
    "<center><img src='img/dendro2.png' width=700></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cophenetic correlation\n",
    "\n",
    "* Cophenetic correlation — correlation between Cophenetic distance and simple distance between objects\n",
    "\n",
    "$$ \\text{cophCorr} = \\frac{\\sum\\limits_{i < j}(d(x_i, x_j) - \\bar{d})(coph(x_i, x_j) - \\bar{coph})}{\\sqrt{\\sum\\limits_{i < j}(d(x_i, x_j) - \\bar{d})^2 \\cdot \\sum\\limits_{i < j}(coph(x_i, x_j) - \\bar{coph})^2}} $$\n",
    "\n",
    "If dendrogram is good, correlation should be high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# А теперь сделаем это с питоном\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage \n",
    "from scipy.cluster.hierarchy import fcluster, cophenet\n",
    "\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "def coph_demo(link='single', metric='euclidean', k=2):\n",
    "    \n",
    "    X, y = make_blobs(n_samples=500,\n",
    "                  n_features=2,\n",
    "                  centers=4,\n",
    "                  cluster_std=1,\n",
    "                  center_box=(-10.0, 10.0),\n",
    "                  shuffle=True,\n",
    "                  random_state=1) \n",
    "    \n",
    "    d = pdist(X, metric=metric)\n",
    "    \n",
    "    Z = linkage(X, method=link, metric=metric)\n",
    "    labels = fcluster(Z,  k, criterion='maxclust')\n",
    "    \n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    \n",
    "    ax[0].scatter(X[:,0], X[:,1], c=labels)\n",
    "        \n",
    "    ax[0].set_xlabel('$x_1$')\n",
    "    ax[0].set_ylabel('$x_2$')\n",
    "        \n",
    "    dend = dendrogram(Z, ax=ax[1], truncate_mode='lastp')   \n",
    "    coph_corr, coph_dist = cophenet(Z, d)\n",
    "    \n",
    "    ax[1].set_title('cophCorr = %.3f' % coph_corr)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327cf41d426245f3ad6ca069663837a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='link', index=1, options=('complete', 'single', 'average', 'centroi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.coph_demo(link='single', metric='euclidean', k=2)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(coph_demo, k=IntSlider(min=2, max=10, step=1, value=2), link=['complete', 'single', 'average', 'centroid'], metric=['euclidean', 'cityblock'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Comments\n",
    "\n",
    "* Results in full hierarchy of objects\n",
    "* Various ways to calculate distances\n",
    "* Nice visualization\n",
    "* Needs a lot of resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Density-based methods\n",
    "# DBScan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Goal\n",
    "\n",
    "* Identify clusters as groups of densly connected points, separated by sparse areas\n",
    "\n",
    "<center><img src='img/dbscan.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Idea\n",
    "\n",
    "* Identify 2 parameters\n",
    "    * $\\epsilon$ - radius of neighbourhood of each point. $N_\\epsilon(p)$ - neighbourhood of point $p$ \n",
    "    * `min_pts` - minimal number of points inside neighbourhood\n",
    "* To start a cluster from point there should be at least `min_pts` points in $N_\\epsilon$\n",
    "* If this conditions is satisfied - spread cluster and check all points neighbours by the same crierion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Types of points\n",
    "* Core point: point having $\\ge \\texttt{min_pts}$ points in its $\\varepsilon$ neighbourhood\n",
    "* Border point: not core point, having at least 1 core point in its $\\varepsilon$ neighbourhood\n",
    "* Noise point: neither a core point nor a border point\n",
    "<center><img src='img/dbscan_points_types.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Demo\n",
    "\n",
    "[Click](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DBSCAN\n",
    "\n",
    "```{C}\n",
    "1.function dbscan(X, eps, min_pts):\n",
    "2.\tinitialize NV = X # not visited objects\t\n",
    "3.\tfor x in NV:\n",
    "4.\t\tremove(NV, x) # mark as visited\n",
    "5.\t\tnbr = neighbours(x, eps) # set of neighbours\n",
    "6.\t\tif nbr.size < min_pts:\n",
    "7.\t\t\tmark_as_noise(x)\n",
    "8.\t\telse:\n",
    "9.\t\t\tC = new_cluster() \n",
    "10.\t\t\texpand_cluster(x, nbr, C, eps, min_pts, NV)\n",
    "11.\t\t\tyield C\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  expand_cluster\n",
    "\n",
    "```{C}\n",
    "1. function expand_cluster(x, nbr, C, eps, min_pts, NV):\n",
    "2.\tadd(x, C)\n",
    "3.\tfor x1 in nbr:\n",
    "4.\t\tif x1 in NV: # object not visited\n",
    "5.\t\t\tremove(NV, x1) # mark as visited\n",
    "6.\t\t\tnbr1 = neighbours(x1, eps)\n",
    "7.\t\t\tif nbr1.size >= min_pts:\n",
    "8.\t\t\t\t# join sets of neighbours\n",
    "9.\t\t\t\tmerge(nbr, nbr_1) \n",
    "10.\t\tif x1 not in any cluster:\n",
    "11.\t\t\tadd(x1, C)\t\t\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "data = np.loadtxt('flame.txt')\n",
    "X_data = data[:, :2]\n",
    "\n",
    "def dbscan_demo(eps=1, min_pts=5):\n",
    "    \n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_pts).fit(X_data)\n",
    "    \n",
    "    labels = dbscan.labels_\n",
    "    \n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.scatter(X_data[:,0], X_data[:, 1], c=labels)\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ca90f671d047a6a921ab02aa36339e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='eps', max=10.0, min=0.1, step=0.05), IntSlider(value…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.dbscan_demo(eps=1, min_pts=5)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(dbscan_demo, eps=FloatSlider(min=0.1, max=10, step=0.05, value=1), min_pts=IntSlider(min=2, max=15, step=1, value=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Recap\n",
    "* Do not indicate number of clusters\n",
    "* Arbitrary shapes of clusters\n",
    "* Identifies outliers\n",
    "* Problems with varying density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Failure for varying density\n",
    "\n",
    "<center><img src='img/diff-dens.png'></center>\n",
    "\n",
    "* Large `min_pts`: cluster C is missed\n",
    "* Small `min_pts`: clusters A and B get merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cluster Validity and Quality Measures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## General approaches\n",
    "* Evaluate using \"ground-truth\" clustering (Quality Measure)\n",
    "    * invariant to cluster naming\n",
    "* Unsupervised criterion (Cluster Validity)\n",
    "    * based on intuition:\n",
    "        * objects from same cluster should be similar\n",
    "        * objects from different clusters should be different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Based on ground-truth\n",
    "#### Rand Index\n",
    "\n",
    "$$ \\text{Rand}(\\hat{\\pi},\\pi^*) = \\frac{a + d}{a + b + c + d} \\text{,}$$\n",
    "where \n",
    "* $a$ - number of pairs that are grouped both in $\\hat{\\pi}$ and\n",
    "* $d$ - number of pairs that are separated both in $\\hat{\\pi}$ and $\\pi^*$\n",
    "$\\pi^*$, \n",
    "* $b$ ($c$) - number of pairs that are separated both in $\\hat{\\pi}$ ($\\pi^*$), but grouped in  $\\pi^*$ ($\\hat{\\pi}$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Rand Index\n",
    "\n",
    "$$ \\text{Rand}(\\hat{\\pi},\\pi^*) = \\frac{tp + tn}{tp + fp + fn + tn} \\text{,}$$\n",
    "where \n",
    "* $tp$ - number of pairs that are grouped both in $\\hat{\\pi}$ and\n",
    "* $tn$ - number of pairs that are separated both in $\\hat{\\pi}$ and $\\pi^*$\n",
    "$\\pi^*$, \n",
    "* $fp$ ($fn$) - number of pairs that are separated both in $\\hat{\\pi}$ ($\\pi^*$), but grouped in  $\\pi^*$ ($\\hat{\\pi}$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Adjusted Rand Index**\n",
    "\n",
    "$$\\text{ARI}(\\hat{\\pi},\\pi^*)   = \\frac{\\text{Rand}(\\hat{\\pi},\\pi^*) - \\text{Expected}}{\\text{Max} - \\text{Expected}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Precision, Recall, F-measure\n",
    "* $\\text{Precision}(\\hat{\\pi},\\pi^*)  = \\frac{tp}{tp+fn}$\n",
    "* $\\text{Recall}(\\hat{\\pi},\\pi^*)  = \\frac{tp}{tp+fp}$\n",
    "* $\\text{F-measure}(\\hat{\\pi},\\pi^*)  = \\frac{2\\cdot Precision \\cdot Recall}{Precision + Recall}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cluster validity\n",
    "* Intuition\n",
    "    * objects from same cluster should be similar\n",
    "    * objects from different clusters should be different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Silhouette\n",
    "\n",
    "For each object $x_{i}$ define:\n",
    "\n",
    "* $s_{i}$-mean distance to objects in the same cluster\n",
    "* $d_{i}$-mean distance to objects in the next nearest cluster\n",
    "Silhouette coefficient for $x_{i}$:\n",
    "$$\n",
    "Silhouette_{i}=\\frac{d_{i}-s_{i}}{\\max\\{d_{i},s_{i}\\}}\n",
    "$$\n",
    "\n",
    "Silhouette coefficient for $x_{1},...x_{N}$:\n",
    "$$\n",
    "Silhouette=\\frac{1}{N}\\sum_{i=1}^{N}\\frac{d_{i}-s_{i}}{\\max\\{d_{i},s_{i}\\}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='http://scikit-learn.org/stable/_images/sphx_glr_plot_kmeans_silhouette_analysis_003.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Discussion\n",
    "* Advantages\n",
    "    * The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. \n",
    "    * Scores around zero indicate overlapping clusters. \n",
    "    * The score is higher when clusters are dense and well separated.\n",
    "\n",
    "* Disadvantages\n",
    "    * complexity $O(N^{2}D)$ \n",
    "    * favours convex clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### FYI\n",
    "* [Quality measures review 1](http://www.sciencedirect.com/science/article/pii/S003132031200338X)\n",
    "* [Quality measures review 2](https://cran.r-project.org/web/packages/clusterCrit/vignettes/clusterCrit.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What else?\n",
    "\n",
    "* Mixture Models (maybe next)\n",
    "* Spectral clustering\n",
    "* Community Detection\n",
    "* Consensus clustering!\n",
    "\n",
    "<center><img src='https://cdn-images-1.medium.com/max/1600/1*fVGpU47jjKRVKdiYqeODkw.png' width=600></center>"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/1f8c4751e12938961e423759861e6e5a"
  },
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "gist": {
   "data": {
    "description": "CloudMail/hse-da-course/raw/lecture-intro/lecture-intro-v01.ipynb",
    "public": false
   },
   "id": "1f8c4751e12938961e423759861e6e5a"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "livereveal": {
   "theme": "serif",
   "transition": "concave"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "513px",
    "width": "253px"
   },
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "toc_position": {
   "height": "32px",
   "left": "9px",
   "right": "1379px",
   "top": "33px",
   "width": "212px"
  },
  "widgets": {
   "state": {
    "54e80d57f79b4bfc934a2b84cf5fe7ba": {
     "views": [
      {
       "cell_index": 47
      }
     ]
    },
    "5fb17a3592634a4fba98446dacd6db43": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "6f6f6ce7b81743308b92966f225862a8": {
     "views": [
      {
       "cell_index": 34
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
