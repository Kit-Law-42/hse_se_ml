{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data from here https://www.kaggle.com/c/boston-housing/overview\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/shestakoff/hse_se_ml/master/2020/s11-boosting/data/boston_train.csv').drop(columns = 'ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Prediction of house price in Boston.\n",
    " \n",
    " Why it's important? Because it can help people determine a fair price at home."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = 'medv').values\n",
    "y = df['medv'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(333, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before doing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "definition of huber_loss: https://en.wikipedia.org/wiki/Huber_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement function which calculate $z_i$ from gradient boosting alogirithm (minus gradient of loss function). Implement for all possible loss function (mse, hl, logloss)\n",
    "\n",
    "In hl use $\\delta = 1$, assume that $MSE = \\frac{1}{2}(r - y)^2$ and log_loss(sigmoid(r)) use $0$ and $1$ like class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_objective(r, target, loss = 'mse'):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "        ----------\n",
    "        r : np.array\n",
    "            value of f(x)\n",
    "        target : np.array\n",
    "            target\n",
    "        loss : str\n",
    "            loss function. Possible values : mse, hl(https://en.wikipedia.org/wiki/Huber_loss, logloss(sigmoid(r))\n",
    "    \"\"\"\n",
    "    return np.zeros(y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test your code\n",
    "\n",
    "r = np.array([1, 2, 3, 4, 5])\n",
    "target = np.array([10, 9, 8, 7, 6])\n",
    "assert(np.allclose(calc_objective(r, target, 'mse'), np.array([9, 7, 5, 3, 1]), 0.00001))\n",
    "\n",
    "r = np.array([2, 4, 7, 9, 13])\n",
    "target = np.array([2.5, 6, 10, 6, 12.75])\n",
    "assert(np.allclose(calc_objective(r, target, 'hl'), np.array([0.5, 1, 1, -1, -0.25]), 0.00001))\n",
    "\n",
    "r = np.array([0, np.log(2), -np.log(9), np.log(4), np.log(19)])\n",
    "target = np.array([0, 1, 0, 1, 1])\n",
    "assert(np.allclose(calc_objective(r, target, 'logloss'), np.array([-0.5, 1. / 3, -0.1, 0.2, 0.05]), 0.00001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement function which make one step of gradient boossting (fit new estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(X, y, r, base_estimator, loss = 'mse'):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "        ----------\n",
    "        X : numpy.array, shape = (n_objects, n_features)\n",
    "            Matrix of objects that are described by their input features.\n",
    "        y : numpy.array, shape = (n_objects)\n",
    "            1D array with the target variables\n",
    "        r : np.array, shape = (n_ojects)\n",
    "            f_{m-1}(X) (X matrix of features) - prediction of previous m-1 base_estimators\n",
    "        base_estimator : estimator which you must fit\n",
    "            has got method fit\n",
    "        loss : str\n",
    "            loss function. Possible values : mse, hl, logloss\n",
    "            \n",
    "        Return fitted base_estimator\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now use your knowledge and implement gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 (6 points)\n",
    "\n",
    "Implement GradientBoostingRegressor as it was described in your lectures:\n",
    "\n",
    "**Input**: training dataset $(x_{i},y_{i}),\\,i=1,2,...N$; loss function $\\mathcal{L}(f,y)$; learning rate $\\nu$ and the number $M$ of successive additive approximations.\n",
    "\n",
    "1. Fit initial approximation $f_{0}(x)$ (might be taken $f_{0}(x)\\equiv0$)\n",
    "2. For each step $m=1,2,...M$:\n",
    "\n",
    "    1. calculate derivatives $z_{i}=-\\frac{\\partial\\mathcal{L}(r,y_{i})}{\\partial r}|_{r=f^{m-1}(x_{i})}$\n",
    "    2. fit $h_{m}$ to $\\{(x_{i},z_{i})\\}_{i=1}^{N}$, for example by solving\n",
    "$$\n",
    "\\sum_{n=1}^{N}(h_{m}(x_{n})-z_{n})^{2}\\to\\min_{h_{m}}\n",
    "$$\n",
    "    4. set $f_{m}(x)=f_{m-1}(x)+\\nu h_{m}(x)$\n",
    "\n",
    "\n",
    "**Output**: approximation function $f_{M}(x)=f_{0}(x)+\\sum_{m=1}^{M}\\nu h_{m}(x)$\n",
    "\n",
    "Implement three loss functions:\n",
    "\n",
    "    1 MSE\n",
    "    2 Huber loss(https://en.wikipedia.org/wiki/Huber_loss)\n",
    "    3 log_loss (in this case we solve classification task\n",
    "In our case $h_m$ is DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to modify this class #\n",
    "\n",
    "class GradientBoostingEstimator(object):\n",
    "\n",
    "    def __init__(self, n_estimators, max_depth = 3, max_leaf_nodes = 8, n_subset_features = 5, random_state = 42,\n",
    "                 loss = 'mse', learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        This is your random forest classifier.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_estimators : int\n",
    "            Number of estimators to train.\n",
    "        max_depth : int\n",
    "            max_depth of DecisionTreeRegressor\n",
    "        max_leaf_nodes:\n",
    "            max_leaf_nodes of DecisionTreeRegressor\n",
    "        n_subset_features : int\n",
    "            Number of random features to used to train a decision tree\n",
    "        random_state : int\n",
    "            random_state for decision tree\n",
    "        loss : str\n",
    "            Loss. Possible values : mse, hl, logloss\n",
    "        learning_rate : float\n",
    "            learning_rate (coef for next estimator on each step)\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the classifier.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.array, shape = (n_objects, n_features)\n",
    "            Matrix of objects that are described by their input features.\n",
    "        y : numpy.array, shape = (n_objects)\n",
    "            1D array with the object labels. \n",
    "            For the classification labels are integers in {0, 1, 2, ...}.\n",
    "        \"\"\"\n",
    "        \n",
    "        pass\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        This methods performs labels prediction for new objects.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.array, shape = (n_objects, n_features)\n",
    "            Matrix of objects that are described by their input features.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        labels : numpy.array, shape = (n_objects)\n",
    "            1D array with predicted labels. \n",
    "            For the classification labels are integers in {0, 1, 2, ...}.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        This methods performs probabilities prediction for new objects.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.array, shape = (n_objects, n_features)\n",
    "            Matrix of objects that are described by their input features.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        proba : numpy.array, shape = (n_objects, n_classes)\n",
    "            Array with predicted probabilities. \n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important\n",
    "When you select `learning_rate` and `n_estimators`, follow the default values of the tree parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 (2 points)\n",
    "\n",
    "Split your data on train, valid sample (fix random_seed). Choose the best `learning_rate` and `n_estimators` for every loss (for logloss use data from previous HW). For regression task (mse_loss, huber_loss) use MSE on valid sample as quality measure to choose optimal params, for classification task (logloss) use ROC-AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 (1 point)\n",
    "Plot dependecy of loss value (in classification task plot roc-auc score) from `n_estimators` of your boosting. Use `learning_rate=0.1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 (1 point)\n",
    "Plot dependecy of loss value (in classification task plot roc-auc score) from `learning_rate` of your boosting. Use `n_estimators=100`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
