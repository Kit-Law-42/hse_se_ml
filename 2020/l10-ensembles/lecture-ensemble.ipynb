{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"img/logo_hse_black.jpg\"></center>\n",
    "\n",
    "<h1><center>Data Analysis</center></h1>\n",
    "<h3><center>Andrey Shestakov (<a href=\"mailto:avshestakov@hse.ru\">avshestakov@hse.ru</a>)</center></h3>\n",
    "<hr>\n",
    "<h2><center>Ensembles.<sup><a href=\"#fn1\" id=\"ref1\">1</a></sup></center></h2>\n",
    "\n",
    "\n",
    "\n",
    "<sup id=\"fn1\">1. Some materials are taken from <a href=\"http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5_%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D1%8B_%D1%80%D0%B0%D1%81%D0%BF%D0%BE%D0%B7%D0%BD%D0%B0%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F_%D0%BE%D0%B1%D1%80%D0%B0%D0%B7%D0%BE%D0%B2_%28%D0%BA%D1%83%D1%80%D1%81_%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D0%B9%2C_%D0%92.%D0%92.%D0%9A%D0%B8%D1%82%D0%BE%D0%B2%29\">machine learning course of Victor Kitov</a></sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-talk')\n",
    "plt.rcParams['figure.figsize'] = (20,12)\n",
    "\n",
    "# Для кириллицы на графиках\n",
    "font = {'family': 'Verdana',\n",
    "        'weight': 'normal'}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "from ipywidgets import interact, IntSlider, fixed, FloatSlider\n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Loss vs model complexity\n",
    "\n",
    "<center><img src='img/loss vs model complexity.png' width=1000></center>\n",
    "\n",
    "* expected loss on test set is always higher than on train set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bias-variance decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bias-variance decomposition\n",
    "\n",
    "* Consider regression task and MSE\n",
    "* Let $y$ - be target features, $x$ - are predictors\n",
    "* True relationship: $ y(x) = f(x) + \\epsilon $ where $ \\epsilon $ is gaussian white noise, $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2) $\n",
    "* Model: $ a(x) = \\hat{y} $.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "Error & = \\mathbb{E}_{X,Y,\\epsilon}\\left[(a(x) - y(x))^2\\right] = \\\\\n",
    "      & = \\mathbb{E}_{X,Y}\\left[(a(x) - \\mathbb{E}_{X,Y}[a(x)])^2\\right]  + \\left(\\mathbb{E}_{X,Y}\\left[a(x)\\right] - f(x)\\right)^2 +  \\sigma^2 = \\\\ & = \\text{Variance} + \\text{Bias}^2  + \\text{Noise}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Those components can be interpreted as\n",
    "* $\\text{Variance}$ (Разброс) - model's sensivity to dataset. High variance usually means that model is overfitted.\n",
    "* $\\text{Bias}$ (Смещение) - is in charge of model's precision. High bias usually means that model is underfitted.\n",
    "* $\\text{Noise}$ (Шум) - that is just noise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bias and variance\n",
    "\n",
    "<center><img src='img/bias-and-variance.jpg' width=900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bias-variance decomposition\n",
    "\n",
    "Define for brevity of notation $y(x)=y,\\, a(x) = a,\\, f(x) = f, \\, \\mathbb{E}=\\mathbb{E}_{X,Y,\\varepsilon}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\text{Error} & = \\mathbb{E}\\left[(a - y)^2\\right] = \\mathbb{E}\\left[a^2 - 2ay + y^2\\right]  = \\\\ \n",
    "             & = \\underbrace{\\mathbb{E}\\left[(a - \\mathbb{E}[a])^2\\right] + \\mathbb{E}[a]^2}_{=\\mathbb{E}[a^2]} - 2\\mathbb{E}[ay] + \\underbrace{\\mathbb{E}\\left[(y - \\mathbb{E}[y])^2\\right] + \\mathbb{E}[y]^2}_{=\\mathbb{E}[y^2]} = \\\\ \n",
    "             & = \\mathbb{E}\\left[(a - \\mathbb{E}[a])^2\\right]  + \\left(\\mathbb{E}\\left[a\\right] - f\\right)^2 +  \\mathbb{E}\\left[(y - f)^2\\right] = \\\\ \n",
    "             & = \\text{Variance} + \\text{Bias}^2  + \\text{Noise}\n",
    "\\end{split} \n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ensemble learning\n",
    "\n",
    "Ensemble learning - using multiple machine learning methods for a given problem and integrating their output to obtain final result\n",
    "\n",
    "**Synonyms:** committee-based learning, multiple classifier systems.\n",
    "\n",
    "**Applications:**\n",
    "* supervised methods: regression, classification\n",
    "* unsupervised methods: clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ensembles use cases\n",
    "\n",
    "* underfitting, *high model bias*\n",
    "    * existing model hypothesis space is too narrow to explain the true\n",
    "one\n",
    "    * different oversimplified models have bias in different directions,\n",
    "mutually compensating each other.\n",
    "\n",
    "* overfitting, *high model variance*\n",
    "    * avoid local optima of optimization methods\n",
    "    * too small dataset to figure out concretely the exact model hypothesis \n",
    "* when task itself promotes usage of ensembles with features of different nature\n",
    "    * E.g. computer security:\n",
    "        * multiple sources of diverse information (password, face detection, fingerprint)\n",
    "        * different abstraction levels need to be united (current action, behavior pattern during day, week, month)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Accuracy improvement demos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Classification\n",
    "<center><img src='img/2 classifiers ensemble - 0.png' width=900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Classification\n",
    "<center><img src='img/2 classifiers ensemble - 1.png' width=900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Classification\n",
    "<center><img src='img/2 classifiers ensemble - 2.png' width=900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Classification\n",
    "<center><img src='img/2 classifiers ensemble - 3.png' width=900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Classification\n",
    "\n",
    "* Consider $M$ classifiers $f_{1}(x),...f_{M}(x)$, performing binary classification. \n",
    "* Let probability of correct precision be constant $p\\in(\\frac{1}{2}, 1)$: $p(f_{m}(x)=y)=p\\,\\forall m$\n",
    "* Suppose all models make mistakes or correct guesses independently of each other. \n",
    "    * Let $F(x)$ be majority voting combiner. \n",
    "    * Then $p(F(x) =  y)\\to1$ as $m\\to\\infty$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68256"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = 5\n",
    "T = M//2 + 1\n",
    "p = 0.6\n",
    "p_ens = sum([scipy.special.comb(M,k)*(p**k)*((1-p)**(M-k)) for k in range(T, M+1)])\n",
    "p_ens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Regression\n",
    "\n",
    "<center><img src='img/2 regressions ensemble - 0.png' width=900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Regression\n",
    "\n",
    "<center><img src='img/2 regressions ensemble - 1.png' width=900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Regression\n",
    "\n",
    "<center><img src='img/2 regressions ensemble - 2.png' width=900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Regression\n",
    "\n",
    "<center><img src='img/2 regressions ensemble - 3.png' width=900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fixed integration schemes for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fixed integration schemes for classification\n",
    "**Output of base learner k:** exact class: $\\omega_{1}$ or $\\omega_{2}$. \n",
    "\n",
    "Combiner predicts $\\omega_{1}$ if:\n",
    "\n",
    "* all classifiers predict $\\omega_{1}$ (AND rule)\n",
    "* at least one classifier predicts $\\omega_{1}$ (OR rule)\n",
    "* at least $k$ classifiers predict $\\omega_{1}$ (k-out-of-N)\n",
    "* majority of classifiers predict $\\omega_{1}$ (majority vote)\n",
    "\n",
    "Each classifier may be assigned a weight, based on its performance:\n",
    "\n",
    "* weighted majority vote\n",
    "* weighted k-out-of-N (based on score sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fixed combiner - ranking level\n",
    "**Output of base learner k:** \n",
    "\n",
    "Ranking of classes:\n",
    "$$\n",
    "\\omega_{k_{1}}\\succeq\\omega_{k_{2}}\\succeq\\ldots\\succeq\\omega_{k_{C}}\n",
    "$$\n",
    "\n",
    "Ranking is equivalent to scoring of each class (with incomparable scoring between classifiers).\n",
    "\n",
    "\n",
    "**Definition**<br/>\n",
    "Let $B_{k}(i)$ be the count of classes scored below $\\omega_{i}$\n",
    "by classifier $k$. \n",
    "\n",
    "*Borda count $B(i)$ of class $\\omega_{i}$* is the total number of classes scored below $\\omega_{i}$ by all classifiers:\n",
    "$$\n",
    "B(i)=\\sum_{k=1}^{K}B_{k}(i)\n",
    "$$\n",
    "\n",
    "Combiner predicts $\\omega_{i}$ where $i=\\arg\\max_{i}B(i)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fixed combiner at class probability level\n",
    "\n",
    "**Output of base learner k:** vectors of class probabilities:\n",
    "$$\n",
    "[p^{k}(\\omega_{1}),\\,p^{k}(\\omega_{2}),\\,\\ldots\\,p^{k}(\\omega_{C})]\n",
    "$$\n",
    "\n",
    "Combiner predicts $\\omega_{i}$ if $i=\\arg\\max_{i}F(p^{1}(\\omega_{i}),p^{2}(\\omega_{i}),\\ldots p^{K}(\\omega_{i}))$\n",
    "* $F$ = mean or median.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sampling ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sampling ensembles\n",
    "\n",
    "<center><img src='img/ensembles.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How bagging affects bias-variance decomposition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Intuitively, bagging should reduce variance\n",
    "\n",
    "* Consider $z_1,\\dots,z_n \\sim Z$, i.i.d., $\\mathbb{E}[Z] = \\mu$, $Var[Z] = \\sigma^2$ \n",
    "* How can we estimate $\\mu$?\n",
    "* We can use any single $z_i$, but it would be quite uncertain, since $Var[z_i] = \\sigma^2$\n",
    "* What about $\\frac{1}{n}\\sum_i z_i$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\mathbb{E}\\left[\\frac{1}{n}\\sum\\limits_i z_i\\right] = \\mu$, but!\n",
    "* $Var\\left[\\frac{1}{n}\\sum\\limits_i z_i\\right] = \\frac{1}{n^2}Var\\left[\\sum\\limits_i z_i\\right] = \\frac{1}{n}\\sigma^2$\n",
    "* Variance of that estimation is lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Say we have $n$ **independent** training sets from the same distribution\n",
    "* Learning $n$ alorithms on them would provide us with $n$ decision functions $a_i(x)$\n",
    "* Average prediction of an object $x$ is\n",
    "$$ a_{avg}(x) = \\frac{1}{n}\\sum_{i=1}^n a_i(x)$$\n",
    "* $\\mathbb{E}[a_{avg}(x)] = \\mathbb{E}[a_1(x)]$\n",
    "* $Var[a_{avg}(x)] = \\frac{1}{n}Var[a_1(x)]$\n",
    "* ...\n",
    "* Profit (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bagging\n",
    "\n",
    "* Need a way to make independent training sets (independent decision functions)\n",
    "* One of the ways is bootstrap: \n",
    "    * Generate $B$ datasets from original $X$ using sampling with replacements - $X_i$\n",
    "    * Each generated dataset contains about $67\\%$ of original dataset\n",
    "* Train a model $a_i(x)$ on each dataset\n",
    "* Average their preditions\n",
    "\n",
    "$$ a_{bag} = \\frac{1}{B}\\sum_{i=1}^B a_i(x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='img/ensembles.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bagging & random subspaces\n",
    "\n",
    "\n",
    "### Bagging\n",
    "\n",
    "* random selection of samples (with replacement)\n",
    "* efficient for methods with high variance w.r.t. $X,Y$.\n",
    "\n",
    "\n",
    "### Random subspace method\n",
    "\n",
    "* random selection of features (without replacement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Random forests\n",
    "\n",
    "<center><img src='img/rf-quote.png' width=800></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Random forests\n",
    "\n",
    "<center><img src='img/rf.png'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Random forests\n",
    "\n",
    "* In fact, bootstraped datasets are not independent w.r.t. $p(x,y)$\n",
    "* Random forests tries to make trees even more independent through randomization in construction algorithm\n",
    "\n",
    "**Input**: \n",
    "* training dataset $TDS=\\{(x_{i},y_{i}),\\,1=1,2,...N\\}$; \n",
    "* the number of trees $B$ and the size of feature subsets $m$.\n",
    "\n",
    "for $b=1,2,...B$:\n",
    "\n",
    "1. generate random training dataset $TDS^{b}$ of size $N$ by sampling $(x_{i},y_{i})$ pairs from $TDS$ with replacement (bootstrap)\n",
    "2. build a tree using $TDS^{b}$ training dataset with feature selection for each node from random subset of features of size $m$ (generated **individually for each node**).\n",
    "\n",
    "\n",
    "**Output**: $B$ trees. Classification is done using majority vote and regression using averaging of $B$ outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Comments\n",
    "* Random forests use random selection on both samples and features \n",
    "* Left out samples may be used for evaluation of model performance.\n",
    "\n",
    "    * *Out-of-bag* prediction: assign output to $x_{i}$, $i=1,2,...N$ using majority vote (classification) or averaging (regression) among trees with $b\\in\\{b:\\,(x_{i},y_{i})\\notin T^{b}\\}$\n",
    "    * *Out-of-bag* quality - lower bound for true model quality.\n",
    "\n",
    "* Less interpretable than individual trees\n",
    "* Parallel implementation\n",
    "* Different trees are not targeted to correct mistakes of each other\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Comments\n",
    "\n",
    "* Extra-Random trees-random sampling of (feature,value) pairs\n",
    "    * more bias and less variance for each tree\n",
    "    * faster training of each tree\n",
    "\n",
    "* RandomForest and ExtraRandomTrees do not overfit with increasing $B$\n",
    "* Each tree should have high depth \n",
    "    * otherwise averaging over oversimplified trees will also give oversimplified model!\n",
    "* [Demo](https://cs.stanford.edu/~karpathy/svmjs/demo/demoforest.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Stacking and blending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Blending\n",
    "\n",
    "<center><img src='img/blending.png' width=900></center>\n",
    "\n",
    "[Alexander Dyakonov (c)](https://alexanderdyakonov.wordpress.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stacking\n",
    "### Weighted averaging\n",
    "\n",
    "Consider regression with $K$ predictor models $f_{k}(x)$, $k=1,2,...K$.\n",
    "\n",
    "(Alternatively we may consider $K$ discriminant functions in classification)\n",
    "\n",
    "**Weighted averaging combiner**<br/>\n",
    "$$\n",
    "f(x)=\\sum_{k=1}^{K}w_{k}f_{k}(x)\n",
    "$$\n",
    "\n",
    "Naive fitting \n",
    "$$\n",
    "\\widehat{w}=\\arg\\min_{w}\\sum_{i=1}^{N}\\mathcal{L}(y_{i},\\sum_{k=1}^{K}w_{k}f_{k}(x_{i}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "overfits. The mostly overfitted method will get the most weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear stacking\n",
    "\n",
    "* Let training set $\\{(x_{i},y_{i}),i=1,2,...N\\}$ be split into $M$\n",
    "folds. \n",
    "* Denote $fold(i)$ to be the fold, containing observation $i$ \n",
    "* Denote $f_{k}^{-fold(i)}$ be predictor $k$ trained on all folds,\n",
    "except $fold(i)$.\n",
    "\n",
    "**Definition**</br>\n",
    "\n",
    "Linear stacking is weighted averaging combiner, where weights are\n",
    "found using\n",
    "$$\n",
    "\\widehat{w}=\\arg\\min_{w}\\sum_{i=1}^{N}\\mathcal{L}(y_{i},\\sum_{k=1}^{K}w_{k}f_{k}^{-fold(i)}(x_{i}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## General stacking\n",
    "\n",
    "<center><img src='img/stacking.png' width=900></center>\n",
    "\n",
    "[Alexander Dyakonov (c)](https://alexanderdyakonov.wordpress.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## General stacking\n",
    "\n",
    "**Definition**\n",
    "\n",
    "Generalized stacking is prediction\n",
    "$$\n",
    "f(x)=A_{\\theta}\\left(f_{1}(x),f_{2}(x),\\ldots f_{K}(x)\\right),\n",
    "$$\n",
    "\n",
    "where $A$ is some general form predictor and $\\theta$ is a vector\n",
    "of parameters, estimated by\n",
    "$$\n",
    "\\widehat{\\theta}=\\arg\\min_{\\theta}\\sum_{i=1}^{N}\\mathcal{L}\\left(y_{i},\\,A_{\\theta}\\left(f_{1}^{-fold(i)}(x),f_{2}^{-fold(i)}(x),\\ldots f_{K}^{-fold(i)}(x)\\right)\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "* Stacking is the most general approach\n",
    "* It is a winning strategy in most ML competitions.\n",
    "* $f_{i}(x)$ may be:\n",
    "    * class number (coded using one-hot encoding).\n",
    "    * vector of class probabilities\n",
    "    * any initial or generated feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* See [this](https://github.com/esokolov/ml-course-hse/blob/master/2017-fall/lecture-notes/lecture08-ensembles.pdf) for additional details\n"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/1f8c4751e12938961e423759861e6e5a"
  },
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "gist": {
   "data": {
    "description": "CloudMail/hse-da-course/raw/lecture-intro/lecture-intro-v01.ipynb",
    "public": false
   },
   "id": "1f8c4751e12938961e423759861e6e5a"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "livereveal": {
   "theme": "serif",
   "transition": "concave"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "513px",
    "width": "253px"
   },
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "toc_position": {
   "height": "32px",
   "left": "9px",
   "right": "1379px",
   "top": "33px",
   "width": "212px"
  },
  "widgets": {
   "state": {
    "54e80d57f79b4bfc934a2b84cf5fe7ba": {
     "views": [
      {
       "cell_index": 47
      }
     ]
    },
    "5fb17a3592634a4fba98446dacd6db43": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "6f6f6ce7b81743308b92966f225862a8": {
     "views": [
      {
       "cell_index": 34
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
