{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"img/logo_hse_black.jpg\"></center>\n",
    "\n",
    "<h1><center>Data Analysis</center></h1>\n",
    "<h3><center>Andrey Shestakov (<a href=\"mailto:avshestakov@hse.ru\">avshestakov@hse.ru</a>)</center></h3>\n",
    "<hr>\n",
    "<h2><center>Neural Networks 2<sup><a href=\"#fn1\" id=\"ref1\">1</a></sup></center></h2>\n",
    "\n",
    "\n",
    "\n",
    "<sup id=\"fn1\">1. Some materials are taken from <a href=\"http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5_%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D1%8B_%D1%80%D0%B0%D1%81%D0%BF%D0%BE%D0%B7%D0%BD%D0%B0%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F_%D0%BE%D0%B1%D1%80%D0%B0%D0%B7%D0%BE%D0%B2_%28%D0%BA%D1%83%D1%80%D1%81_%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D0%B9%2C_%D0%92.%D0%92.%D0%9A%D0%B8%D1%82%D0%BE%D0%B2%29\">machine learning course of Victor Kitov</a></sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('seaborn-talk')\n",
    "plt.rcParams['figure.figsize'] = (12,8)\n",
    "\n",
    "# Для кириллицы на графиках\n",
    "font = {'family': 'Verdana',\n",
    "        'weight': 'normal'}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "from ipywidgets import interact, IntSlider, fixed, FloatSlider\n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's recall previous lecture\n",
    "\n",
    "## Feed-Forward Nets\n",
    "\n",
    "* NNs have started long time ago\n",
    "* Composed of layers\n",
    "    * Layers composed of neurons\n",
    "* Each layers sum up outputs of previous layer and put it throught some non-linear activation function\n",
    "* Backpropagation algorithm\n",
    "\n",
    "<center><img src='img/neuron-diagram.jpg'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is image?\n",
    "\n",
    "<center><img src='img/image.png' width=700></center>\n",
    "\n",
    "* Multi-dimentional array\n",
    "* Key characteristics:\n",
    "    * width\n",
    "    * height\n",
    "    * depth\n",
    "    * range\n",
    "* Example\n",
    "    * RGB images have depth of 3 (one for each of R, G, B)\n",
    "    * Usually color intencity range is from 0 to 255 (8 bits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Common Tasks\n",
    "\n",
    "<center><img src='img/tasks.jpeg' width=1200></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Computer Vision (CV)\n",
    "* There were (are) many things before CNN Hype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Threshold segmentation\n",
    "\n",
    "<center><img src='img/ada_thresh.png' width=1200></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Threshold segmentation\n",
    "\n",
    "<center><img src='img/ada_thresh2.jpg' width=900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Template Matching\n",
    "\n",
    "<center><img src='img/matching.png', width=1200></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Object Detection with Viola-Jones and Haar\n",
    "\n",
    "<center><img src='img/haar_features.jpg' width=600></center>\n",
    "\n",
    "<center><img src='img/haar_features2.png' width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Object Detection with Viola-Jones and Haar\n",
    "\n",
    "<center><img src='img/face_detect.png' width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Edge Detection - Sobel Operator\n",
    "\n",
    "<center><img src='img/sobel.png' width=700></center>\n",
    "\n",
    "<center><img src='img/sobel2.png' width=900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## It is actually a convolution filter!\n",
    "\n",
    "<center><img src='img/convolution1.gif', width=900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Result\n",
    "\n",
    "<center><img src='img/sobel3.png' width=900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conv Nets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Typical CNN Architecture\n",
    "\n",
    "<center><img src='img/typical_cnn_architecture.png', width=1200></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Convolution Filters\n",
    "\n",
    "<center><img src='img/conv1.png', width=900>></center>\n",
    "\n",
    "(c) [cs231n](http://cs231n.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Convolution Filters\n",
    "\n",
    "<center><img src='img/conv2.png', width=900></center>\n",
    "\n",
    "(c) [cs231n](http://cs231n.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Convolution Filters\n",
    "\n",
    "<center><img src='img/conv3.png', width=900></center>\n",
    "\n",
    "(c) [cs231n](http://cs231n.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Features\n",
    "\n",
    "<center><img src='img/cnn_features.png', width=900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Key Specs of Conv Filters\n",
    "\n",
    "* Number of filters\n",
    "* Filter Size ($K$)\n",
    "* Padding Size ($P$)\n",
    "* Stride Size ($S$)\n",
    "\n",
    "If number of input features is $N_{in}$ how to calculate number of output features $N_{out}$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$N_{out} = \\frac{N_{in} + 2P - K}{S} + 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Number of parameters to learn?\n",
    "\n",
    "* Input: $32 \\times 32 \\times 3$\n",
    "* $20$ Filters: $5 \\times 5$, $P = 1$, $S = 2$\n",
    "* Number of weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$20\\times(5 \\times 5 \\times 3 + 1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Backprop through convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='img/conv_example.png', width=1300></center>\n",
    "\n",
    "(c) [cs231n](http://cs231n.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Lets simplify things a bit: consider only a single channel $5 \\times 5$ input and $3 \\times 3$ convolutional filter. \n",
    "\n",
    "No strides, no padding, no bias weight.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left( \\begin{array}{ccccc} \n",
    "0 & 1 & 2 & 1 & 0 \\\\ \n",
    "4 & 1 & 0 & 1 & 0 \\\\ \n",
    "2 & 0 & 1 & 1 & 1 \\\\ \n",
    "1 & 2 & 3 & 1 & 0 \\\\ \n",
    "0 & 4 & 3 & 2 & 0 \\\\ \n",
    "\\end{array} \\right) &\\quad * &\n",
    "\\left( \\begin{array}{ccc} \n",
    "0 & 1 & 0 \\\\ \n",
    "1 & 0 & 1 \\\\ \n",
    "2 & 1 & 0 \\\\ \n",
    "\\end{array} \\right) & \\quad = &\n",
    "\\left( \\begin{array}{ccc} \n",
    "9 & 5 & 4 \\\\\n",
    "8 & 8 & 10 \\\\\n",
    "8 & 15 & 12 \\\\\n",
    "\\end{array} \\right) \\\\\n",
    "\\mathbf{X} \\qquad \\qquad &\\quad * & \\mathbf{W} \\qquad & \\quad = & \\mathbf{I} \\qquad\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Denote\n",
    "* $I_{i,j}$ - input $(i,j)$-th element of feature map (\"neuron\")\n",
    "* $O_{i,j}$ - \"activated\" $(i,j)$-th element of feature map (\"neuron\")\n",
    "\n",
    "Go down to indices (filter indexing starts from \"center\"):\n",
    "\n",
    "$$ I_{i,j} = \\sum\\limits_{-1 \\leq a,b \\leq 1} W_{a,b}X_{i+a, j+b} $$\n",
    "\n",
    "And usually some activation function $f(\\cdot)$ is applied: \n",
    "$$ O_{i,j} = f(I_{i,j}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similarly to backprop algorithm in previous lecture:\n",
    "* Define $\\delta_{i,j}$ as error in $(i,j)$ position of feature map:\n",
    "$$ \\delta_{i,j} = \\frac{\\partial L}{\\partial I_{i,j}} =  \\frac{\\partial L}{\\partial O_{i,j}}\\frac{\\partial O_{i,j}}{\\partial I_{i,j}} = \\frac{\\partial L}{\\partial O_{i,j}} f'(I_{i,j}) $$\n",
    "* To compute $\\frac{\\partial L}{\\partial W_{a,b}}$ we have to take into account that weight as \"shared\"\n",
    "$$\\frac{\\partial L}{\\partial W_{a,b}} = \\sum_i \\sum_j \\frac{\\partial L}{\\partial I_{i,j}} \\frac{\\partial I_{i,j}}{\\partial W_{a,b}} = \\sum_i \\sum_j \\delta_{i,j} X_{i+a, j+b}$$\n",
    "* Finally, to pass gradients to previous layers:\n",
    "$$\\frac{\\partial L}{\\partial X_{i,j}} = \\sum_{a,b} \\frac{\\partial L}{\\partial I_{i-a, j-b}} \\frac{\\partial I_{i-a, j-b}}{\\partial X_{i, j}} = \\sum_{a,b} \\delta_{i-a,j-b} W_{a,b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "OMG, it is convolution too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pooling layers\n",
    "\n",
    "<center><img src='img/pooling1.png', width=900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pooling layers\n",
    "\n",
    "<center><img src='img/pooling2.png', width=900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What's with derivative here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Common Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## AlexNet\n",
    "\n",
    "<center><img src='img/AlexNet.png', width=1200></center>\n",
    "\n",
    "\n",
    "* Max-pooling\n",
    "* Re-LU activation\n",
    "* 60M weights\n",
    "* First place in the 2012 ImageNet competition\n",
    "* ConvNets hype begins!\n",
    "* [Paper](https://www.nvidia.cn/content/tesla/pdf/machine-learning/imagenet-classification-with-deep-convolutional-nn.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## LeNet5 (1998)\n",
    "\n",
    "<center><img src='img/lenet5.png', width=1500></center>\n",
    "\n",
    "* Hand-written characters recognition\n",
    "* Avg-Pooling\n",
    "* 60 000 weights\n",
    "* [Paper](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## VGG 16\n",
    "<center><img src='img/vgg16.png', width=900></center>\n",
    "\n",
    "* Cascades of smaller convolutions instead of larger ones\n",
    "* 140 M parameters\n",
    "* Freezing weights during learning\n",
    "* [Paper](https://arxiv.org/abs/1409.1556)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Google Inception\n",
    "\n",
    "<center><img src='img/inception-model.png', width=900></center>\n",
    "\n",
    "* Cascades of smaller convolutions instead of larger ones\n",
    "* 5 (V1) parameters\n",
    "* Introduction of batch normalization and residual blocks\n",
    "* Additional loss segments\n",
    "* [Paper](https://arxiv.org/pdf/1409.4842.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ResNet\n",
    "\n",
    "<center><img src='img/resnet-model.png', width=1200></center>\n",
    "\n",
    "* ULTRA DEEP\n",
    "* Residual connections\n",
    "* [Paper](https://arxiv.org/abs/1512.03385)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## [ConvNN Demo](https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ConvNets for text\n",
    "\n",
    "<center><img src='img/text_conv.png', width=1200></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Autoencoders\n",
    "\n",
    "* Autoencoders tries to recover input signal from compressed representation \n",
    "    * so they should save only the most valuable inforation!\n",
    "    * dimention reduction!\n",
    "\n",
    "<center><img src='img/autoencoder_network1.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Autoencoders\n",
    "\n",
    "<center><img src='img/autoencoder_schema.jpg'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Thats all folks!"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/1f8c4751e12938961e423759861e6e5a"
  },
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "gist": {
   "data": {
    "description": "CloudMail/hse-da-course/raw/lecture-intro/lecture-intro-v01.ipynb",
    "public": false
   },
   "id": "1f8c4751e12938961e423759861e6e5a"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "livereveal": {
   "theme": "serif",
   "transition": "concave"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "513px",
    "width": "253px"
   },
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "toc_position": {
   "height": "32px",
   "left": "9px",
   "right": "1379px",
   "top": "33px",
   "width": "212px"
  },
  "widgets": {
   "state": {
    "54e80d57f79b4bfc934a2b84cf5fe7ba": {
     "views": [
      {
       "cell_index": 47
      }
     ]
    },
    "5fb17a3592634a4fba98446dacd6db43": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "6f6f6ce7b81743308b92966f225862a8": {
     "views": [
      {
       "cell_index": 34
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
