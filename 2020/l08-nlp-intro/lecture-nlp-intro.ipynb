{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"img/logo_hse_black.jpg\"></center>\n",
    "\n",
    "<h1><center>Data Analysis</center></h1>\n",
    "<h3><center>Andrey Shestakov (<a href=\"mailto:avshestakov@hse.ru\">avshestakov@hse.ru</a>)</center></h3>\n",
    "<hr>\n",
    "<h2><center>Introduction to Natural Language Processing<sup><a href=\"#fn1\" id=\"ref1\">1</a></sup></center></h2>\n",
    "\n",
    "\n",
    "\n",
    "<sup id=\"fn1\">1. Some materials are taken from <a href=\"http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5_%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D1%8B_%D1%80%D0%B0%D1%81%D0%BF%D0%BE%D0%B7%D0%BD%D0%B0%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F_%D0%BE%D0%B1%D1%80%D0%B0%D0%B7%D0%BE%D0%B2_%28%D0%BA%D1%83%D1%80%D1%81_%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D0%B9%2C_%D0%92.%D0%92.%D0%9A%D0%B8%D1%82%D0%BE%D0%B2%29\">machine learning course of Victor Kitov</a></sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-talk')\n",
    "plt.rcParams['figure.figsize'] = (15,10)\n",
    "\n",
    "# Для кириллицы на графиках\n",
    "font = {'family': 'Verdana',\n",
    "        'weight': 'normal'}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "try:\n",
    "    from ipywidgets import interact, IntSlider, fixed, FloatSlider\n",
    "except ImportError:\n",
    "    print(u'Так надо')\n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Huge amount of information is represented in a form of natual language:\n",
    "    * Blog-posts\n",
    "    * Comments\n",
    "    * Review\n",
    "    * Search queries\n",
    "    * Code\n",
    "    * ...\n",
    "\n",
    "* Need a way to process this information:\n",
    "    * transfer \"characters\" to vectors\n",
    "    * recognize key entities in text\n",
    "    * understand relationships between them\n",
    "    * ...\n",
    "\n",
    "* **Computational linguistics** — is an interdisciplinary field concerned with the statistical or rule-based modeling of natural language from a computational perspective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Applications\n",
    "\n",
    "* Machine Translation\n",
    "* Information retrieval\n",
    "* Text summarization\n",
    "* Text Classification\n",
    "    * Spam filtering\n",
    "    * Semantic analysis\n",
    "    * Multilabel classification\n",
    "* Text Categorization\n",
    "* Named-entity recognition\n",
    "* QA-systems and assistants\n",
    "* Text Generation\n",
    "* Spell-checking and next-word suggestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Machine Translation\n",
    "\n",
    "<center><img src='http://lurkmore.so/images/0/04/Epic_fail.jpg' width=900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Named Entity Recognition\n",
    "\n",
    "<center><img src='https://hsto.org/getpro/habr/post_images/b73/6f7/a6c/b736f7a6c182da3285c81fcb8ce32d85.jpg' width=900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## QA-systems and assistants\n",
    "\n",
    "<center><img src='https://cdn5.img.ria.ru/images/150662/35/1506623555.jpg'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Text suggestion\n",
    "\n",
    "<center><img src='./img/search.png', width=1200></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Text Generation\n",
    "\n",
    "* https://talktotransformer.com/\n",
    "* https://pdos.csail.mit.edu/archive/scigen/\n",
    "* https://sgeneri.ru/text-generator\n",
    "* https://yandex.ru/referats/\n",
    "* https://yandex.ru/autopoet/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# NLP is HARD!\n",
    "* Various linguistic uncertainties\n",
    "    * morphologic: \"мой\", \"три\", \"стекло\"\n",
    "    * phonetic: \"Надо ждать\" - \"Надо ж дать\"\n",
    "    * lexical: \"кран\", \"ключ\"\n",
    "    * syntactical: \"мужу изменять нельзя\"\n",
    "* Language is in dynamic!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Levels of abstraction\n",
    "* Characters\n",
    "* Words\n",
    "* Sentences\n",
    "* Paragraphs\n",
    "* Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Tokenization\n",
    "\n",
    "* **Tokenization** - task of document decomposition on to **tokens** (elemental units)\n",
    "* Usually, word-based granularity is considered\n",
    "\n",
    "\n",
    "* \"Кролики - это не только ценный мех, но и 3-4 килограмма диетического, легкоусвояемого мяса\"\n",
    "* \"Через 1.5 часа поеду в Гусь-Хрустальный.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Tokenization of words\n",
    "\n",
    "* Special **regular expressions** can be utilized to word this out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "line = u\"Через 1.5 часа поеду в Гусь-Хрустальный.\"\n",
    "for w in line.split(' '):\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "line = u\"Через 1.5 часа поеду в Гусь-Хрустальный.\"\n",
    "tokenizer = RegexpTokenizer('\\w+| \\$ [\\d \\.]+ | S\\+')\n",
    "for w in tokenizer.tokenize(line):\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "line = u\"Через 1.5 часа поеду в Гусь-Хрустальный.\"\n",
    "for w in wordpunct_tokenize(line):\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "line = u'テキストで表示ロシア語'\n",
    "line = u'Nach der Wahrscheinlichkeitstheorie steckt alles in Schwierigkeiten!'\n",
    "\n",
    "# =("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Tokenization of sentences\n",
    "* Also not clear\n",
    "* May use regex\n",
    "* Or some model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = 'Good muffins cost $3.88 in New York. Please buy me two of them. Thanks.'\n",
    "sent_tokenize(text, language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "text = u\"Через 1.5 часа поеду в Гусь-Хрустальный. Куплю там квасу!\"\n",
    "for sent in sent_tokenize(text, language='english'):\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## n-grams\n",
    "\n",
    "* Sometimes it is usefull to consider sequences of tokens\n",
    "* To catch context\n",
    "* To be robust to spelling errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def word_grams(words, min_=1, max_=4):\n",
    "    s = []\n",
    "    for n in range(min_, max_):\n",
    "        for ngram in ngrams(words, n):\n",
    "            s.append(u\" \".join(str(i) for i in ngram))\n",
    "    return s\n",
    "\n",
    "word_grams(u\"I prefer cheese sause\".split(u\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n = 3\n",
    "line = \"cheese sause\"\n",
    "\n",
    "for i in range(len(line) - n + 1):\n",
    "    print(line[i:i+n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Byte-pair encoding\n",
    "\n",
    "1. Prepare a large enough training data (i.e. corpus)\n",
    "2. Define a desired subword vocabulary size\n",
    "3. Split words to sequence of characters\n",
    "4. Produce new subword as concatination of most frequent pair of characters\n",
    "5. Recalculate frequences and repeat \"4\" until desired vocalularly size is met\n",
    "\n",
    "`I saw a girl with a telescope.` ->  \n",
    "`['▁I', '▁saw', '▁a', '▁girl', '▁with', '▁a', '▁', 'te', 'le', 's', 'c', 'o', 'pe', ‘.’]`\n",
    "\n",
    "* [SentencePiece](https://github.com/google/sentencepiece)\n",
    "* [FastBPE](https://github.com/glample/fastBPE)\n",
    "* [YouTokenToMe](https://github.com/vkcom/YouTokenToMe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Two types of text normalization\n",
    "\n",
    "* **Lemmatization** - process of transformation of a word to its base form (normal form) based on dictionaries morphological analysis\n",
    "    * mystem, pymorphy\n",
    "* **Stemming** - rule-based process of reduction of a words to its stem based\n",
    "    * sses → ss (caresses → caress)\n",
    "    * ies → i (ponies → poni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "           'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "           'meeting', 'stating', 'siezing', 'itemization',\n",
    "           'sensational', 'traditional', 'reference', 'colonizer',\n",
    "           'plotted']\n",
    "\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "m = Mystem()\n",
    "text = 'в петербурге прошел митинг против передачи исаакиевского собора рпц'\n",
    "\n",
    "print(''.join(m.lemmatize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pymorphy2 \n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "res = morph.parse(u'пожарница')\n",
    "for item in res:\n",
    "    print('====')\n",
    "    print(u'norm_form: {}'.format(item.normal_form))\n",
    "    print(u'tag: {}'.format(item.tag))\n",
    "    print(u'score: {}'.format(item.score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Other preprocessing techniques\n",
    "* high-frequency word (stop-words) removal\n",
    "    * Unions, prepositions, numerals, pronouns\n",
    "* low-frequency word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('russian')\n",
    "print(stopwords[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Other preprocessing techniques\n",
    "* Switching to lowercase\n",
    "    * may loose meaning (US > us) or emotion (GREAT > great)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Document vectorization\n",
    "# Bag Of Words (BOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Bag of words\n",
    "\n",
    "* Most common way to represent documents\n",
    "* Each **document** $d_i$ from **corpus** $D$ is represented with vector\n",
    "$$ d_i = (w_1^i, w_2^i, \\dots, w_N^i) $$\n",
    "* $w_j^i$ - weight of word $j$ in document $i$\n",
    "* word order is ignored\n",
    "\n",
    "<center><img src='img/bow.png'><center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Bag of words\n",
    "\n",
    "$w_j^i$ can be calculated as\n",
    "* Binary signal (does document contain the word)\n",
    "* Frequency of word $j$ in document $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tf - Idf\n",
    "\n",
    "* tf-idf (term frequency - inversed document frequency)\n",
    "    * tf(term, doc)\n",
    "        * Binary signal\n",
    "        * Frequency of word $j$ in document $i$\n",
    "        * Smoothed frequency $\\log(f_j^i + 1)$\n",
    "    * idf(term, corpus) = inversed document frequency if a word $j$ in corpus $D$: $\\log\\left(\\frac{|D|}{n_j} + 1\\right)$\n",
    "        * $|D|$ - size of corpus\n",
    "        * $n_t$ - number of documents which contain word $j$\n",
    "        * idf measures how specifit a word is\n",
    "        \n",
    "\n",
    "$$tfidf = tf(term,doc) * idf(term, corpus)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='img/tfidf.jpg', width=1200><center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bag of words\n",
    "\n",
    "* Additionaly, l1 or l2 normalization is applied to document vectors\n",
    "    * Normalization is usually good for models\n",
    "    * Maintain \"meaning\" of a document (for longer documents words' frequency are usually greater)\n",
    "\n",
    "\n",
    "* Ok, what we can do now with BoW?\n",
    "    * Put it as features in models (linear models, naive bayes)\n",
    "    * Use it to find similar documents (with cosine or jaccard distance)\n",
    "    * Compress it to \"topics\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Latent Semantic Indexing - LSI\n",
    "### aka\n",
    "## Latent Semantic Analysis - LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## LSA and feature reduction technique\n",
    "\n",
    "Idea (and implementation) of LSI is very similar to PCA\n",
    "* We want to compress sparse BOW-representation in to dense representation\n",
    "* We want this representation to save as much initial information and be meaningful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Singular Value Decomposition (SVD)\n",
    "\n",
    "Every matrix $A$ of size $n \\times m$ and rank $r$ can be decomposed as:\n",
    "$$ A = U \\Sigma V^\\top ,$$\n",
    "where \n",
    "* $U$ - unitary matrix, consists of eigenvectors of $AA^\\top$\n",
    "* $V$ - unitary matrix, consists of eigenvectors of $A^\\top A$\n",
    "* $\\Sigma$ - diagonal matrix with singular values $s_i = \\sqrt{\\lambda_i}$\n",
    "\n",
    "<center><img src='img/svd.jpg' width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## SVD via PCA\n",
    "Matrices $U$ and $V$ can be used for PCA\n",
    "$$ AV = U\\Sigma $$\n",
    "\n",
    "<center><img src='img/pca_svd.png' width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## LSA\n",
    "\n",
    "* Let's make SVD of BOW matrix\n",
    "\n",
    "<center><img src='img/lsa.jpg'><center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Pros**\n",
    "* Easy\n",
    "* Can find similar words with $V$ and similar documents with $U$\n",
    "* LSA in search task is called Latent Semantic Indexing\n",
    "    * Query $q$ is also some sequence of words\n",
    "    * How to we find most similar documents in corpus with LSI?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Pros**\n",
    "* Simple\n",
    "* Can find similar words with $V$ and similar documents with $U$\n",
    "* LSA in search task is called Latent Semantic Indexing\n",
    "    * Query $q$ is also some sequence of words\n",
    "    * Pass q through the same preprocessing as main text\n",
    "    * Make BOW representation of it\n",
    "    * Make LSI representation with\n",
    "        $$ \\hat{q} = qV\\Sigma^{-1} $$\n",
    "\n",
    "**Cons**\n",
    "* Problems with interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "texts = [\n",
    "    \"Группа крови на рукаве\",\n",
    "    \"Мой порядковый номер на рукаве\",\n",
    "    \"Пожелай мне удачи в бою\",\n",
    "    \"Пожелай мне не остаться в этой траве\",\n",
    "    \"Не остаться в этой траве\",\n",
    "    \"Пожелай мне удачи\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X = TfidfVectorizer(norm=None).fit_transform(texts, )\n",
    "print(X.toarray().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "Z = TruncatedSVD(n_components=3).fit_transform(X)[:, 1:]\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,1)\n",
    "ax.scatter(Z[:, 0], Z[:, 1])\n",
    "ax.scatter(0,0)\n",
    "for i, z in enumerate(Z):\n",
    "    ax.arrow(0., 0., z[0], z[1], width=0.001)\n",
    "    ax.text(z[0], z[1], texts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* LSA is just some strange dimention-reduction algorithm which is applied to data\n",
    "* If we consider topic modelling as a phenomenon - it is much deeper than simple compression\n",
    "\n",
    "<center><img src='img/LDA-concept.png'><center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probabilistic topic models\n",
    "\n",
    "<center><img src='img/topic-meme.jpg' width=900></cetner>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# But.. not today..\n",
    "\n",
    "## FYI\n",
    "* [David Blei - topic modelling](https://www.youtube.com/watch?v=DDq3OVp9dNA)\n",
    "* [Tutorial on probabilistic TM](http://machinelearning.ru/wiki/images/1/1f/Voron14aist.pdf)\n",
    "* [Gensim - LDA](https://radimrehurek.com/gensim/models/ldamodel.html)\n",
    "* [BigARTM](https://bigartm.readthedocs.io/en/stable/)"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/1f8c4751e12938961e423759861e6e5a"
  },
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "gist": {
   "data": {
    "description": "CloudMail/hse-da-course/raw/lecture-intro/lecture-intro-v01.ipynb",
    "public": false
   },
   "id": "1f8c4751e12938961e423759861e6e5a"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "livereveal": {
   "theme": "serif",
   "transition": "concave"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "513px",
    "width": "253px"
   },
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "toc_position": {
   "height": "32px",
   "left": "9px",
   "right": "1379px",
   "top": "33px",
   "width": "212px"
  },
  "widgets": {
   "state": {
    "54e80d57f79b4bfc934a2b84cf5fe7ba": {
     "views": [
      {
       "cell_index": 47
      }
     ]
    },
    "5fb17a3592634a4fba98446dacd6db43": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "6f6f6ce7b81743308b92966f225862a8": {
     "views": [
      {
       "cell_index": 34
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
